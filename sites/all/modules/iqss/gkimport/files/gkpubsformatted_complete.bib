@article {HopKin10,
	title = {A Method of Automated Nonparametric Content Analysis for Social Science},
	journal = {American Journal of Political Science},
	volume = {54},
	number = {1},
	year = {2010},
	month = {January},
	pages = {229{\textendash}247},
	abstract = {The increasing availability of digitized text presents enormous opportunities for social scientists. Yet hand coding many blogs, speeches, government records, newspapers, or other sources of unstructured text is infeasible. Although computer scientists have methods for automated content analysis, most are optimized to classify individual documents, whereas social scientists instead want generalizations about the population of documents, such as the proportion in a given category. Unfortunately, even a method with a high percent of individual documents correctly classified can be hugely biased when estimating category proportions. By directly optimizing for this social science goal, we develop a method that gives approximately unbiased estimates of category proportions even when the optimal classifier performs poorly. We illustrate with diverse data sets, including the daily expressed opinions of thousands of people about the U.S. presidency. We also make available software that implements our methods and large corpora of text for further analysis.},
	url = {http://gking.harvard.edu/files/abs/words-abs.shtml},
	author = {Daniel Hopkins and Gary King}
}
@article {GriKin10,
	title = {Quantitative Discovery from Qualitative Information: A General-Purpose Document Clustering Methodology},
	year = {2010},
	abstract = {Many people attempt to discover useful information by reading large quantities of unstructured text, but because of known human limitations even experts are ill-suited to succeed at this task. This difficulty has inspired the creation of numerous automated cluster analysis methods to aid discovery. We address two problems that plague this literature. First, the optimal use of any one of these methods requires that it be applied only to a specific substantive area, but the best area for each method is rarely discussed and usually unknowable ex ante. We tackle this problem with mathematical, statistical, and visualization tools that define a search space built from the solutions to all previously proposed cluster analysis methods (and any qualitative approaches one has time to include) and enable a user to explore it and quickly identify useful information. Second, in part because of the nature of unsupervised learning problems, cluster analysis methods are not routinely evaluated in ways that make them vulnerable to being proven suboptimal or less than useful in specific data types. We therefore propose new experimental designs for evaluating these methods. With such evaluation designs, we demonstrate that our computer-assisted approach facilitates more efficient and insightful discovery of useful information than either expert human coders using qualitative or quantitative approaches or existing automated methods. We (will) make available an easy-to-use software package that implements all our suggestions.},
	url = {http://gking.harvard.edu/files/abs/discov-abs.shtml},
	author = {Justin Grimmer and Gary King}
}
@article {HonKin10,
	title = {What to do About Missing Values in Time Series Cross-Section Data},
	journal = {American Journal of Political Science},
	year = {2010},
	abstract = {Applications of modern methods for analyzing data with missing values, based primarily on multiple imputation, have in the last half-decade become common in American politics and political behavior. Scholars in these fields have thus increasingly avoided the biases and inefficiencies caused by ad hoc methods like listwise deletion and best guess imputation. However, researchers in much of comparative politics and international relations, and others with similar data, have been unable to do the same because the best available imputation methods work poorly with the time-series cross-section data structures common in these fields. We attempt to rectify this situation. First, we build a multiple imputation model that allows smooth time trends, shifts across cross-sectional units, and correlations over time and space, resulting in far more accurate imputations. Second, we build nonignorable missingness models by enabling analysts to incorporate knowledge from area studies experts via priors on individual missing cell values, rather than on difficult-to-interpret model parameters. Third, since these tasks could not be accomplished within existing imputation algorithms, in that they cannot handle as many variables as needed even in the simpler cross-sectional data for which they were designed, we also develop a new algorithm that substantially expands the range of computationally feasible data types and sizes for which multiple imputation can be used. These developments also made it possible to implement the methods introduced here in freely available open source software that is considerably more reliable than existing strategies.},
	url = {http://gking.harvard.edu/files/abs/pr-abs.shtml},
	author = {James Honaker and Gary King}
}
@article {HonKinBLa09,
	title = {Amelia II: A Program for Missing Data},
	year = {2009},
	author = {James Honaker and Gary King and Matthew Blackwell}
}
@article {IacKinPor09,
	title = {Causal Inference Without Balance Checking: Coarsened Exact Matching},
	year = {2009},
	abstract = {We address a major discrepancy in matching methods for causal inference in observational data. Since these data are typically plentiful, the goal of matching is to reduce bias and only secondarily to keep variance low. However, most matching methods seem designed for the opposite goal, guaranteeing sample size ex ante but limiting bias by controlling for covariates through reductions in the imbalance between treated and control groups only ex post and only sometimes. (The resulting practical difficulty may explain why so many published applications do not check whether imbalance was reduced and so may not even be decreasing bias.) We introduce {\textquoteleft}{\textquoteleft}Coarsened Exact Matching{\textquoteright}{\textquoteright} (CEM) which, unlike most existing approaches, bounds through ex ante user choice the degree of maximal imbalance, model dependence, and causal effect estimation error; eliminates the need for a separate procedure to restrict data to common support; meets the congruence principle; is approximately invariant to measurement error; works well with multicategory treatment variables and with modern methods of imputation for missing data; is computationally efficient even with massive data sets; and is easy to understand and use. CEM can improve causal inferences in a wide range of applications, and may be preferred for simplicity of use even when it is possible to design superior methods for particular problems. We also make available open source software for R and Stata which implements all our suggestions.},
	url = {http://gking.harvard.edu/files/abs/cem-plus-abs.shtml},
	author = {Stefano M. Iacus and Gary King and Giuseppe Porro}
}
@article {IacKinPor09b,
	title = {CEM: Coarsened Exact Matching Software},
	journal = {Journal of statistical Software},
	volume = {30},
	year = {2009},
	url = {http://gking.harvard.edu/cem},
	author = {Stefano M. Iacus and Gary King and Giuseppe Porro}
}
@article {ImaKinNal09,
	title = {The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation},
	journal = {Statistical Science},
	volume = {24},
	number = {1},
	year = {2009},
	pages = {29{\textendash}53},
	abstract = {A basic feature of many field experiments is that investigators are only able to randomize clusters of individuals --- such as households, communities, firms, medical practices, schools, or classrooms --- even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, in part because of claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; and its standard error is also invalid. To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. We show that from the perspective of bias, efficiency, power, robustness, or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one{\textquoteright}s data. We develop these techniques in the context of a randomized evaluation we are conducting of the Mexican Universal Health Insurance Program.},
	url = {http://gking.harvard.edu/files/abs/cluster-abs.shtml},
	author = {Kosuke Imai and Gary King and Clayton Nall}
}
@article {KinSon09,
	title = {The Future of Death in America},
	year = {2009},
	note = {http://gking.harvard.edu/files/abs/mort-abs.shtml},
	abstract = {Population mortality forecasts, despite their uncertainties, are widely used for allocating public health expenditures, setting research priorities, and evaluating the viability of Social Security, private pensions, and health care financing systems. Although we know a great deal about patterns in and causes of mortality rates, most existing forecasts are still based on simple linear extrapolations that ignore covariates and other prior information. We adapt a Bayesian hierarchical forecasting model capable of including more known biological and demographic information than has previously been possible. This leads to the first age- and sex-specific forecasts of American mortality that simultaneously incorporates in a formal statistical model the effects of the recent rapid increase in obesity, the steady decline in tobacco consumption, and the well known patterns of smooth mortality age profiles and time trends. Formally including new information in forecasts can matter a great deal. For example, we estimate an increase in male life expectancy at birth from 75.1 years to 79.8 years over the next quarter-century, which is 1.7 years greater than the Social Security Administration projection and 1.4 years more than U.S. Census projection. For females, we estimate more modest gains in life expectancy at birth over the next quarter-century from 80.2 years to 82.7 years, which is 0.8 years greater than the Social Security Administration projection and 0.4 years less than U.S. census projections. We show that these patterns are also likely to greatly affect the aging American population structure. We offer sophisticated, but easy-to-use, methods so that researchers can include other sources of information and potentially improve on our forecasts.},
	author = {Gary King and Samir Soneji}
}
@inbook {King09b,
	title = {The Future of Political Science: 100 Perspectives},
	year = {2009},
	publisher = {Routledge Press},
	organization = {Routledge Press},
	chapter = {The Changing Evidence Base of Political Science Research},
	address = {New York},
	abstract = {This (two-page) article argues that the evidence base of political science and the related social sciences are beginning an underappreciated but historic change.},
	url = {http://gking.harvard.edu/files/abs/evbase-abs.shtml},
	author = {Gary King},
	editor = {Gary King and Kay Schlozman and Norman Nie}
}
@article {HopKin09b,
	title = {Improving Anchoring Vignettes: Designing Surveys to Correct Interpersonal Incomparability},
	journal = {Public Opinion Quarterly},
	year = {2009},
	abstract = {We report the results of several randomized survey experiments designed to evaluate two intended improvements to anchoring vignettes, an increasingly common technique used to achieve interpersonal comparability in survey research.~ This technique asks for respondent self-assessments followed by assessments of hypothetical people described in vignettes. Variation in assessments of the vignettes across respondents reveals interpersonal incomparability and allows researchers to make responses more comparable by rescaling them. Our experiments show, first, that switching the question order so that self-assessments follow the vignettes primes respondents to define the response scale in a common way.~ In this case, priming is not a bias to avoid but a means of better communicating the question{\textquoteright}s meaning.~ We then demonstrate that combining vignettes and self-assessments in a single direct comparison induces inconsistent and less informative responses.~ Since similar combined strategies are widely employed for related purposes, our results indicate that anchoring vignettes could reduce measurement error in many applications where they are not currently used.~ Data for our experiments come from a national telephone survey and a separate on-line survey.},
	url = {http://gking.harvard.edu/files/abs/implement-abs.shtml},
	author = {Daniel Hopkins and Gary King}
}
@article {ImaKinNal09d,
	title = {Matched Pairs and the Future of Cluster-Randomized Experiments: A Rejoinder},
	journal = {Statistical Science},
	volume = {24},
	number = {1},
	year = {2009},
	pages = {64{\textendash}72},
	abstract = {A basic feature of many field experiments is that investigators are only able to randomize clusters of individuals --- such as households, communities, firms, medical practices, schools, or classrooms --- even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, in part because of claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; and its standard error is also invalid. To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. We show that from the perspective of bias, efficiency, power, robustness, or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one{\textquoteright}s data. We develop these techniques in the context of a randomized evaluation we are conducting of the Mexican Universal Health Insurance Program.},
	author = {Kosuke Imai and Gary King and Clayton Nall}
}
@article {IacKinPor09a,
	title = {Multivariate Matching Methods That are Monotonic Imbalance Bounding},
	year = {2009},
	abstract = {We introduce a new "Monotonic Imbalance Bounding" (MIB) class of matching methods for causal inference with a surprisingly large number of attractive statistical properties. MIB generalizes and extends in several new directions the only existing class, "Equal Percent Bias Reducing" (EPBR), which is designed to satisfy weaker properties and only in expectation. We also offer strategies to obtain specific members of the MIB class, and analyze in more detail a member of this class, called Coarsened Exact Matching, whose properties we analyze from this new perspective. We offer a variety of analytical results and numerical simulations that demonstrate how members of the MIB class can dramatically improve inferences relative to EPBR-based matching methods.},
	url = {http://gking.harvard.edu/files/abs/cem-math-abs.shtml},
	author = {Stefano M. Iacus and Gary King and Giuseppe Porro}
}
@article {KinGakIma09,
	title = {Public Policy for the Poor? A Randomised Assessment of the Mexican Universal Health Insurance Programme},
	journal = {The Lancet},
	year = {2009},
	abstract = {Background

We assessed aspects of Seguro Popular, a programme aimed to deliver health insurance, regular and preventive medical care, medicines, and health facilities to 50 million uninsured Mexicans.

Methods

We randomly assigned treatment within 74 matched pairs of health clusters -- ie, health facility catchment areas -- representing 118,569 households in seven Mexican states, and measured outcomes in a 2005 baseline survey (August, 2005, to September, 2005) and follow-up survey 10 months later (July, 2006, to August, 2006) in 50 pairs (n=32 515). The treatment consisted of encouragement to enrol in a health-insurance programme and upgraded medical facilities. Participant states also received funds to improve health facilities and to provide medications for services in treated clusters. We estimated intention to treat and complier average causal effects non-parametrically.

Findings

Intention-to-treat estimates indicated a 23\% reduction from baseline in catastrophic expenditures (1{\textperiodcentered}9\% points; 95\% CI 0{\textperiodcentered}14-3{\textperiodcentered}66). The effect in poor households was 3{\textperiodcentered}0\% points (0{\textperiodcentered}46-5{\textperiodcentered}54) and in experimental compliers was 6{\textperiodcentered}5\% points (1{\textperiodcentered}65-11{\textperiodcentered}28), 30\% and 59\% reductions, respectively. The intention-to-treat effect on health spending in poor households was 426 pesos (39-812), and the complier average causal effect was 915 pesos (147-1684). Contrary to expectations and previous observational research, we found no effects on medication spending, health outcomes, or utilisation.

Interpretation

Programme resources reached the poor. However, the programme did not show some other effects, possibly due to the short duration of treatment (10 months). Although Seguro Popular seems to be successful at this early stage, further experiments and follow-up studies, with longer assessment periods, are needed to ascertain the long-term effects of the programme.},
	url = {http://gking.harvard.edu/files/abs/spi-abs.shtml},
	author = {Gary King and Emmanuela Gakidou and Kosuke Imai and Jason Lakin and Ryan T. Moore and Clayton Nall and Nirmala Ravishankar and Manett Vargas and Martha Mar{\'\i}a T{\'e}llez-Rojo and Juan Eugenio Hern{\'a}ndez {\'A}vila and Mauricio Hern{\'a}ndez {\'A}vila and H{\'e}ctor Hern{\'a}ndez Llamas}
}
@article {LazPenAda09,
	title = {SOCIAL SCIENCE: Computational Social Science},
	journal = {Science},
	volume = {323},
	number = {5915},
	year = {2009},
	pages = {721-723},
	abstract = {A field is emerging that leverages the capacity to collect and analyze data at a scale that may reveal patterns of individual and group behaviors.},
	url = {http://gking.harvard.edu/files/abs/LazPenAda09-abs.shtml},
	author = {Lazer, David and Pentland, Alex and Adamic, Lada and Aral, Sinan and Barabasi, Albert-Laszlo and Brewer, Devon and Christakis, Nicholas and Contractor, Noshir and Fowler, James and Gutmann, Myron and Jebara, Tony and Gary King and Macy, Michael and Roy, Deb and Van Alstyne, Marshall}
}
@article {KinLuShi09,
	title = {Designing Verbal Autopsy Studies},
	year = {2009},
	author = {Gary King and Ying Lu and Kenji Shibuya}
}
@book {KinSchNie09,
	title = {The Future of Political Science: 100 Perspectives},
	year = {2009},
	publisher = {Routledge Press},
	organization = {Routledge Press},
	address = {New York},
	editor = {Gary King and Kay Schlozman and Norman Nie}
}
@article {AbrBolGut09,
	title = {Preserving Data for Long Term Analyses},
	journal = {Historical Social Research},
	year = {2009},
	author = {Mark Abrahamson and Kenneth A. Bollen and Gutmann, Myron and Gary King and Amy M. Pienta}
}
@article {HopKin09b,
	title = {Replication Data for: A Method of Automated Nonparametric Content Analysis for Social Science},
	year = {2009},
	note = {\underline{UNF:3:xlE5stLgKvpeMvxzlLxzEQ==} hdl:1902.1/12898 Murray Research Archive [Distributor]},
	author = {Daniel Hopkins and Gary King}
}
@article {KinGakIma09b,
	title = {Replication Data for: Public Policy for the Poor? A Randomized Ten-Month Evaluation of the Mexican Universal Health Insurance Program},
	year = {2009},
	note = {\underline{hdl:1902.1/11044} UNF:3:jeUN9XODtYUp2iUbe8gWZQ== Murray Research Archive [Distributor]},
	author = {Gary King and Emmanuela Gakidou and Kosuke Imai and Jason Lakin and Clayton Nall and Ryan T. Moore and Nirmala Ravishankar and Manett Vargas and Martha Mar{\'\i}a T{\'e}llez-Rojo and Juan Eugenio Hern{\'a}ndez {\'A}vila and Mauricio Hern{\'a}ndez {\'A}vila and H{\'e}ctor Hern{\'a}ndez Llamas}
}
@article {ImaKinNal09b,
	title = {Replication Data for: The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation},
	year = {2009},
	note = {\underline{hdl:1902.1/11047} UNF:3:jeUN9XODtYUp2iUbe8gWZQ== Murray Research Archive [Distributor]},
	author = {Kosuke Imai and Gary King and Clayton Nall}
}
@article {ImaKinNal09c,
	title = {Replication Data for: The Essential Role of Pair-Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation: Rejoinder},
	year = {2009},
	note = {\underline{hdl:1902.1/12730} UNF:3:CKs4T0iVYxP36LQSMgAkuw== Murray Research Archive [Distributor]},
	author = {Kosuke Imai and Gary King and Clayton Nall}
}
@book {GirKin08,
	title = {Demographic Forecasting},
	year = {2008},
	publisher = {Princeton University Press},
	organization = {Princeton University Press},
	address = {Princeton},
	url = {http://gking.harvard.edu/files/smooth},
	author = {Federico Girosi and Gary King}
}
@article {ImaKinStu08,
	title = {Misunderstandings Among Experimentalists and Observationalists about Causal Inference},
	journal = {Journal of the Royal Statistical Society, Series A},
	volume = {171, part 2},
	year = {2008},
	pages = {481{\textendash}502},
	abstract = {We attempt to clarify, and suggest how to avoid, several serious misunderstandings about and fallacies of causal inference in experimental and observational research. These issues concern some of the most basic advantages and disadvantages of each basic research design. Problems include improper use of hypothesis tests for covariate balance between the treated and control groups, and the consequences of using randomization, blocking before randomization, and matching after treatment assignment to achieve covariate balance. Applied researchers in a wide range of scientific disciplines seem to fall prey to one or more of these fallacies, and as a result make suboptimal design or analysis choices. To clarify these points, we derive a new four-part decomposition of the key estimation errors in making causal inferences. We then show how this decomposition can help scholars from different experimental and observational research traditions better understand each other{\textquoteright}s inferential problems and attempted solutions.},
	url = {http://gking.harvard.edu/files/abs/matchse-abs.shtml},
	author = {Kosuke Imai and Gary King and Elizabeth Stuart}
}
@article {ImaKinLau07,
	title = {Toward A Common Framework for Statistical Analysis and Development},
	journal = {Journal of Computational Graphics and Statistics},
	volume = {17},
	number = {4},
	year = {2008},
	pages = {1{\textendash}22},
	abstract = {We describe some progress toward a common framework for statistical analysis and software development built on and within the R language, including R{\textquoteright}s numerous existing packages. The framework we have developed offers a simple unified structure and syntax that can encompass a large fraction of statistical procedures already implemented in R, without requiring any changes in existing approaches. We conjecture that it can be used to encompass and present simply a vast majority of existing statistical methods, regardless of the theory of inference on which they are based, notation with which they were developed, and programming syntax with which they have been implemented. This development enabled us, and should enable others, to design statistical software with a single, simple, and unified user interface that helps overcome the conflicting notation, syntax, jargon, and statistical methods existing across the methods subfields of numerous academic disciplines. The approach also enables one to build a graphical user interface that automatically includes any method encompassed within the framework. We hope that the result of this line of research will greatly reduce the time from the creation of a new statistical innovation to its widespread use by applied researchers whether or not they use or program in R.},
	url = {http://gking.harvard.edu/files/abs/z-abs.shtml},
	author = {Kosuke Imai and Gary King and Olivia Lau}
}
@article {KinLu08,
	title = {Verbal Autopsy Methods with Multiple Causes of Death},
	journal = {Statistical Science},
	volume = {23},
	number = {1},
	year = {2008},
	pages = {78{\textendash}91},
	abstract = {Verbal autopsy procedures are widely used for estimating cause-specific mortality in areas without medical death certification. Data on symptoms reported by caregivers along with the cause of death are collected from a medical facility, and the cause-of-death distribution is estimated in the population where only symptom data are available. Current approaches analyze only one cause at a time, involve assumptions judged difficult or impossible to satisfy, and require expensive, time consuming, or unreliable physician reviews, expert algorithms, or parametric statistical models. By generalizing current approaches to analyze multiple causes, we show how most of the difficult assumptions underlying existing methods can be dropped. These generalizations also make physician review, expert algorithms, and parametric statistical assumptions unnecessary. With theoretical results, and empirical analyses in data from China and Tanzania, we illustrate the accuracy of this approach. While no method of analyzing verbal autopsy data, including the more computationally intensive approach offered here, can give accurate estimates in all circumstances, the procedure offered is conceptually simpler, less expensive, more general, as or more replicable, and easier to use in practice than existing approaches. We also show how our focus on estimating aggregate proportions, which are the quantities of primary interest in verbal autopsy studies, may also greatly reduce the assumptions necessary, and thus improve the performance of, many individual classifiers in this and other areas. As a companion to this paper, we also offer easy-to-use software that implements the methods discussed herein.},
	url = {http://gking.harvard.edu/files/abs/vamc-abs.shtml},
	author = {Gary King and Ying Lu}
}
@article {KinZen08,
	title = {Replication data for: Empirical vs. Theoretical Claims about Extreme Counterfactuals: A Response},
	year = {2008},
	note = {\underline{hdl:1902.1/11903}, Murray Research Archive [Distributor]},
	author = {Gary King and Langche Zeng}
}
@article {KinWan07,
	title = {Comparing Incomparable Survey Responses: New Tools for Anchoring Vignettes},
	journal = {Political Analysis},
	volume = {15},
	number = {1},
	year = {2007},
	month = {Winter},
	pages = {46-66},
	abstract = {When respondents use the ordinal response categories of standard survey questions in different ways, the validity of analyses based on the resulting data can be biased. Anchoring vignettes is a survey design technique, introduced by King, Murray, Salomon, and Tandon (2004), intended to correct for some of these problems. We develop new methods both for evaluating and choosing anchoring vignettes, and for analyzing the resulting data. With surveys on a diverse range of topics in a range of countries, we illustrate how our proposed methods can improve the ability of anchoring vignettes to extract information from survey data, as well as saving in survey administration costs.},
	url = {http://gking.harvard.edu/files/abs/c-abs.shtml},
	author = {Gary King and Jonathan Wand}
}
@article {AltKin07,
	title = {A Proposed Standard for the Scholarly Citation of Quantitative Data},
	journal = {D-Lib Magazine},
	volume = {13},
	number = {3/4},
	year = {2007},
	month = {March / April},
	abstract = {An essential aspect of science is a community of scholars cooperating and competing in the pursuit of common goals. A critical component of this community is the common language of and the universal standards for scholarly citation, credit attribution, and the location and retrieval of articles and books. We propose a similar universal standard for citing quantitative data that retains the advantages of print citations, adds other components made possible by, and needed due to, the digital form and systematic nature of quantitative data sets, and is consistent with most existing subfield-specific approaches. Although the digital library field includes numerous creative ideas, we limit ourselves to only those elements that appear ready for easy practical use by scientists, journal editors, publishers, librarians, and archivists.},
	url = {http://gking.harvard.edu/files/abs/cite-abs.shtml},
	author = {Micah Altman and Gary King}
}
@article {KinZen07b,
	title = {Detecting Model Dependence in Statistical Inference: A Response},
	journal = {International Studies Quarterly},
	volume = {51},
	year = {2007},
	month = {March},
	pages = {231-241},
	abstract = {Inferences about counterfactuals are essential for prediction, answering "what if" questions, and estimating causal effects. However, when the counterfactuals posed are too far from the data at hand, conclusions drawn from well-specified statistical analyses become based on speculation and convenient but indefensible model assumptions rather than empirical evidence. Unfortunately, standard statistical approaches assume the veracity of the model rather than revealing the degree of model-dependence, and so this problem can be hard to detect. We develop easy-to-apply methods to evaluate counterfactuals that do not require sensitivity testing over specified classes of models. If an analysis fails the tests we offer, then we know that substantive results are sensitive to at least some modeling choices that are not based on empirical evidence. We use these methods to evaluate the extensive scholarly literatures on the effects of changes in the degree of democracy in a country (on any dependent variable) and separate analyses of the effects of UN peacebuilding efforts. We find evidence that many scholars are inadvertently drawing conclusions based more on modeling hypotheses than on their data. For some research questions, history contains insufficient information to be our guide.},
	url = {http://gking.harvard.edu/files/abs/counterf-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {KinZen07,
	title = {When Can History Be Our Guide? The Pitfalls of Counterfactual Inference},
	journal = {International Studies Quarterly},
	year = {2007},
	month = {March},
	pages = {183-210},
	abstract = {Inferences about counterfactuals are essential for prediction, answering "what if" questions, and estimating causal effects. However, when the counterfactuals posed are too far from the data at hand, conclusions drawn from well-specified statistical analyses become based on speculation and convenient but indefensible model assumptions rather than empirical evidence. Unfortunately, standard statistical approaches assume the veracity of the model rather than revealing the degree of model-dependence, and so this problem can be hard to detect. We develop easy-to-apply methods to evaluate counterfactuals that do not require sensitivity testing over specified classes of models. If an analysis fails the tests we offer, then we know that substantive results are sensitive to at least some modeling choices that are not based on empirical evidence. We use these methods to evaluate the extensive scholarly literatures on the effects of changes in the degree of democracy in a country (on any dependent variable) and separate analyses of the effects of UN peacebuilding efforts. We find evidence that many scholars are inadvertently drawing conclusions based more on modeling hypotheses than on their data. For some research questions, history contains insufficient information to be our guide.},
	url = {http://gking.harvard.edu/files/abs/counterf-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {GroKin07,
	title = {The Future of Partisan Symmetry as a Judicial Test for Partisan Gerrymandering after LULAC v. Perry},
	journal = {Election Law Journal},
	volume = {6},
	number = {1},
	year = {2007},
	month = {January},
	pages = {2-35},
	abstract = {While the Supreme Court in Bandemer v. Davis found partisan gerrymandering to be justiciable, no challenged redistricting plan in the subsequent 20 years has been held unconstitutional on partisan grounds. Then, in Vieth v. Jubilerer, five justices concluded that some standard might be adopted in a future case, if a manageable rule could be found. When gerrymandering next came before the Court, in LULAC v. Perry, we along with our colleagues filed an Amicus Brief (King et al., 2005), proposing the test be based in part on the partisan symmetry standard. Although the issue was not resolved, our proposal was discussed and positively evaluated in three of the opinions, including the plurality judgment, and for the first time for any proposal the Court gave a clear indication that a future legal test for partisan gerrymandering will likely include partisan symmetry. A majority of Justices now appear to endorse the view that the measurement of partisan symmetry may be used in partisan gerrymandering claims as {\textquotedblleft}a helpful (though certainly not talismanic) tool{\textquotedblright} (Justice Stevens, joined by Justice Breyer), provided one recognizes that {\textquotedblleft}asymmetry alone is not a reliable measure of unconstitutional partisanship{\textquotedblright} and possibly that the standard would be applied only after at least one election has been held under the redistricting plan at issue (Justice Kennedy, joined by Justices Souter and Ginsburg). We use this essay to respond to the request of Justices Souter and Ginsburg that {\textquotedblleft}further attention {\textellipsis} be devoted to the administrability of such a criterion at all levels of redistricting and its review.{\textquotedblright} Building on our previous scholarly work, our Amicus Brief, the observations of these five Justices, and a supporting consensus in the academic literature, we offer here a social science perspective on the conceptualization and measurement of partisan gerrymandering and the development of relevant legal rules based on what is effectively the Supreme Court{\textquoteright}s open invitation to lower courts to revisit these issues in the light of LULAC v. Perry.},
	url = {http://gking.harvard.edu/files/abs/jp-abs.shtml},
	author = {Bernard Grofman and Gary King}
}
@article {KinGakRav07,
	title = {A "Politically Robust" Experimental Design for Public Policy Evaluation, with Application to the Mexican Universal Health Insurance Program},
	journal = {Journal of Policy Analysis and Management},
	volume = {26},
	number = {3},
	year = {2007},
	pages = {479-506},
	abstract = {We develop an approach to conducting large scale randomized public policy experiments intended to be more robust to the political interventions that have ruined some or all parts of many similar previous efforts. Our proposed design is insulated from selection bias in some circumstances even if we lose observations; our inferences can still be unbiased even if politics disrupts any two of the three steps in our analytical procedures; and other empirical checks are available to validate the overall design. We illustrate with a design and empirical validation of an evaluation of the Mexican Seguro Popular de Salud (Universal Health Insurance) program we are conducting. Seguro Popular, which is intended to grow to provide medical care, drugs, preventative services, and financial health protection to the 50 million Mexicans without health insurance, is one of the largest health reforms of any country in the last two decades. The evaluation is also large scale, constituting one of the largest policy experiments to date and what may be the largest randomized health policy experiment ever.},
	url = {http://gking.harvard.edu/files/abs/spd-abs.shtml},
	author = {Gary King and Emmanuela Gakidou and Nirmala Ravishankar and Ryan T. Moore and Jason Lakin and Manett Vargas and Martha Mar{\'\i}a T{\'e}llez-Rojo and Juan Eugenio Hern{\'a}ndez {\'A}vila and Mauricio Hern{\'a}ndez {\'A}vila and H{\'e}ctor Hern{\'a}ndez Llamas}
}
@article {King07,
	title = {An Introduction to the Dataverse Network as an Infrastructure for Data Sharing},
	journal = {Sociological Methods and Research},
	volume = {36},
	number = {2},
	year = {2007},
	pages = {173{\textendash}199},
	abstract = {We introduce a set of integrated developments in web application software, networking, data citation standards, and statistical methods designed to put some of the universe of data and data sharing practices on somewhat firmer ground. We have focused on social science data, but aspects of what we have developed may apply more widely. The idea is to facilitate the public distribution of persistent, authorized, and verifiable data, with powerful but easy-to-use technology, even when the data are confidential or proprietary. We intend to solve some of the sociological problems of data sharing via technological means, with the result intended to benefit both the scientific community and the sometimes apparently contradictory goals of individual researchers.},
	url = {http://gking.harvard.edu/files/abs/dvn-abs.shtml},
	author = {Gary King}
}
@article {HoImaKin07,
	title = {Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference},
	journal = {Political Analysis},
	volume = {15},
	year = {2007},
	pages = {199{\textendash}236},
	abstract = {Although published works rarely include causal estimates from more than a few model specifications, authors usually choose the presented estimates from numerous trial runs readers never see. Given the often large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? How do readers know that publications are not merely demonstrations that it is \emph{possible} to find a specification that fits the author{\textquoteright}s favorite hypothesis? And how do we evaluate or even define statistical properties like unbiasedness or mean squared error when no unique model or estimator even exists? Matching methods, which offer the promise of causal inference with fewer assumptions, constitute one possible way forward, but crucial results in this fast-growing methodological literature are often grossly misinterpreted. We explain how to avoid these misinterpretations and propose a unified approach that makes it possible for researchers to preprocess data with matching (such as with the easy-to-use software we offer) and then to apply the best parametric techniques they would have used anyway. This procedure makes parametric models produce more accurate and considerably less model-dependent causal inferences.},
	url = {http://gking.harvard.edu/files/abs/matchp-abs.shtml},
	author = {Daniel Ho and Kosuke Imai and Gary King and Elizabeth Stuart}
}
@article {HoImaKin07a,
	title = {MatchIt: Nonparametric Preprocessing for Parametric Causal Inference},
	journal = {Journal of Statistical Software},
	year = {2007},
	url = {http://gking.harvard.edu/matchit},
	author = {Daniel E. Ho and Kosuke Imai and Gary King and Elizabeth A. Stuart}
}
@article {KinRosTan07,
	title = {Ordinary Economic Voting Behavior in the Extraordinary Election of Adolf Hitler},
	year = {2007},
	abstract = {The enormous Nazi voting literature rarely builds on modern statistical or economic research. By adding these approaches, we find that the most widely accepted existing theories of this era cannot distinguish the Weimar elections from almost any others in any country. Via a retrospective voting account, we show that voters most hurt by the depression, and most likely to oppose the government, fall into separate groups with divergent interests. This explains why some turned to the Nazis and others turned away. The consequences of Hitler{\textquoteright}s election were extraordinary, but the voting behavior that led to it was not.},
	url = {http://gking.harvard.edu/files/abs/naziV-abs.shtml},
	author = {Gary King and Ori Rosen and Martin Tanner and Alexander F. Wagner.}
}
@article {GirKin07,
	title = {Understanding the Lee-Carter Mortality Forecasting Method},
	year = {2007},
	abstract = {We demonstrate here several previously unrecognized or insufficiently appreciated properties of the Lee-Carter mortality forecasting approach, the dominant method used in both the academic literature and practical applications. We show that this model is a special case of a considerably simpler, and less often biased, random walk with drift model, and prove that the age profile forecast from both approaches will always become less smooth and unrealistic after a point (when forecasting forward or backwards in time) and will eventually deviate from any given baseline. We use these and other properties we demonstrate to suggest when the model would be most applicable in practice.},
	url = {http://gking.harvard.edu/files/abs/lc-abs.shtml},
	author = {Federico Girosi and Gary King}
}
@article {WanKinLau07,
	title = {Anchors: Software for Anchoring Vignettes Data},
	journal = {Journal of Statistical Software},
	year = {2007},
	author = {Jonathan Wand and Gary King and Olivia Lau}
}
@article {King06,
	title = {Publication, Publication},
	journal = {PS: Political Science and Politics},
	volume = {39},
	number = {01},
	year = {2006},
	month = {January},
	pages = {119{\textendash}125},
	abstract = {I show herein how to write a publishable paper by beginning with the replication of a published article. This strategy seems to work well for class projects in producing papers that ultimately get published, helping to professionalize students into the discipline, and teaching them the scientific norms of the free exchange of academic information. I begin by briefly revisiting the prominent debate on replication our discipline had a decade ago and some of the progress made in data sharing since.},
	url = {http://gking.harvard.edu/files/abs/paperspub-abs.shtml},
	author = {Gary King}
}
@article {GakKin06,
	title = {Death by Survey: Estimating Adult Mortality without Selection Bias from Sibling Survival Data from Sibling Survival Data},
	journal = {Demography},
	volume = {43},
	number = {3},
	year = {2006},
	month = {August},
	pages = {569{\textendash}585},
	abstract = {The widely used methods for estimating adult mortality rates from sample survey responses about the survival of siblings, parents, spouses, and others depend crucially on an assumption that we demonstrate does not hold in real data. We show that when this assumption is violated -- so that the mortality rate varies with sibship size -- mortality estimates can be massively biased. By using insights from work on the statistical analysis of selection bias, survey weighting, and extrapolation problems, we propose a new and relatively simple method of recovering the mortality rate with both greatly reduced potential for bias and increased clarity about the source of necessary assumptions.},
	url = {http://gking.harvard.edu/files/abs/deathbys-abs.shtml},
	author = {Emmanuela Gakidou and Gary King}
}
@article {KinZen06,
	title = {The Dangers of Extreme Counterfactuals},
	journal = {Political Analysis},
	volume = {14},
	number = {2},
	year = {2006},
	pages = {131{\textendash}159},
	abstract = {We address the problem that occurs when inferences about counterfactuals -- predictions, "what if" questions, and causal effects -- are attempted far from the available data. The danger of these extreme counterfactuals is that substantive conclusions drawn from statistical models that fit the data well turn out to be based largely on speculation hidden in convenient modeling assumptions that few would be willing to defend. Yet existing statistical strategies provide few reliable means of identifying extreme counterfactuals. We offer a proof that inferences farther from the data are more model-dependent, and then develop easy-to-apply methods to evaluate how model-dependent our answers would be to specified counterfactuals. These methods require neither sensitivity testing over specified classes of models nor evaluating any specific modeling assumptions. If an analysis fails the simple tests we offer, then we know that substantive results are sensitive to at least some modeling choices that are not based on empirical evidence.},
	url = {http://gking.harvard.edu/files/abs/counterft-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@inbook {KinRosTan06,
	title = {The New Palgrave Dictionary of Economics},
	year = {2006},
	edition = {2nd},
	chapter = {Ecological Inference},
	abstract = {Dictionary entry on the definition of "ecological inference," and a brief summary of the history of ecological inference research.},
	url = {http://gking.harvard.edu/files/abs/newintro-abs.shtml},
	author = {Gary King and Ori Rosen and Martin Tanner},
	editor = {Larry Blume and Steven N. Durlauf}
}
@inbook {EpsHoKin06,
	title = {Principles and Practice in American Politics: Classic and Contemporary Readings},
	year = {2006},
	publisher = {Congressional Quarterly Press},
	organization = {Congressional Quarterly Press},
	edition = {3rd},
	chapter = {The Effect of War on the Supreme Court},
	address = {Washington, D.C.},
	abstract = {Does the U.S. Supreme Court curtail rights and liberties when the nation{\textquoteright}s security is under threat? In hundreds of articles and books, and with renewed fervor since September 11, 2001, members of the legal community have warred over this question. Yet, not a single large-scale, quantitative study exists on the subject. Using the best data available on the causes and outcomes of every civil rights and liberties case decided by the Supreme Court over the past six decades and employing methods chosen and tuned especially for this problem, our analyses demonstrate that when crises threaten the nation{\textquoteright}s security, the justices are substantially more likely to curtail rights and liberties than when peace prevails. Yet paradoxically, and in contradiction to virtually every theory of crisis jurisprudence, war appears to affect only cases that are unrelated to the war. For these cases, the effect of war and other international crises is so substantial, persistent, and consistent that it may surprise even those commentators who long have argued that the Court rallies around the flag in times of crisis. On the other hand, we find no evidence that cases most directly related to the war are affected. We attempt to explain this seemingly paradoxical evidence with one unifying conjecture: Instead of balancing rights and security in high stakes cases directly related to the war, the Justices retreat to ensuring the institutional checks of the democratic branches. Since rights-oriented and process-oriented dimensions seem to operate in different domains and at different times, and often suggest different outcomes, the predictive factors that work for cases unrelated to the war fail for cases related to the war. If this conjecture is correct, federal judges should consider giving less weight to legal principles outside of wartime but established during wartime, and attorneys should see it as their responsibility to distinguish cases along these lines.},
	author = {Lee Epstein and Daniel E. Ho and Gary King and Jeffrey A. Segal},
	editor = {Samuel Kernell and Steven S. Smith}
}
@article {ImaKinLau06,
	title = {Zelig: Everyone{\textquoteright}s Statistical Software},
	year = {2006},
	url = {http://gking.harvard.edu/zelig},
	author = {Kosuke Imai and Gary King and Olivia Lau}
}
@article {GirKin06,
	title = {Cause of Death Data},
	year = {2006},
	note = {{\underline{hdl:1902.1/UOVMCPSWOL} UNF:3:9JU+SmVyHgwRhAKclQ85Cg== Murray Research Archive [Distributor]}},
	author = {Federico Girosi and Gary King}
}
@article {KinAlt06,
	title = {Replication data for: A Unified Model of Cabinet Dissolution in Parliamentary Democracies},
	year = {2006},
	note = {{\underline{hdl:1902.1/RMPXNUSBBS} UNF:3:lfKIeFJKgejkOzXEY1i6lw== Murray Research Archive [Distributor]}},
	author = {Gary King and James E. Alt and Nancy Burns and Michael Laver}
}
@article {GakKin06b,
	title = {Replication data for: Death by Survey: Estimating Adult Mortality without Selection Bias from Sibling Survival Data},
	year = {2006},
	note = {{\underline{hdl:1902.1/ZMESWNECZW} Murray Research Archive [Distributor]}},
	author = {Emmanuela Gakidou and Gary King}
}
@article {KinZen06c,
	title = {Replication data for: Detecting Model Depedence in Statistical Inference: A Response},
	year = {2006},
	note = {{\underline{hdl:1902.1/FGSRBXXIYT} UNF:3:K4/CgnMYDMV6izc5RVOZTA== Murray Research Archive [Distributor]}},
	author = {Gary King and Langche Zeng}
}
@article {KinZen06e,
	title = {Replication data for: The Dangers of Extreme Counterfactuals},
	year = {2006},
	note = {{\underline{hdl:1902.1/UTVMBVNGMX} UNF:3:ytKKNjK+yR8Pq3H0RcV6eg== Murray Research Archive [Distributor]}},
	author = {Gary King and Langche Zeng}
}
@article {KinZen06d,
	title = {Replication data for: When Can History be Our Guide? The Pitfalls of Counterfactual Inference},
	year = {2006},
	note = {{\underline{hdl:1902.1/DXRXCFAWPK} UNF:3:DaYlT6QSX9r0D50ye+tXpA== Murray Research Archive [Distributor]}},
	author = {Gary King and Langche Zeng}
}
@article {HoImaKin06,
	title = {Replication Data Set for: Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference},
	year = {2006},
	note = {{\underline{hdl:1902.1/YVDZEQIYDS} Murray Research Archive [distributor]}},
	author = {Daniel E. Ho and Kosuke Imai and Gary King and Elizabeth A. Stuart}
}
@article {KinZen06b,
	title = {Replication Data Set for: When Can History be Our Guide? The Pitfalls of Counterfactual Inference},
	year = {2006},
	note = {{\underline{hdl:1902.1/DXRXCFAWPK} Murray Research Archive [distributor]}},
	author = {Gary King and Langche Zeng}
}
@article {EpsHoKin05,
	title = {The Supreme Court During Crisis: How War Affects only Non-War Cases},
	journal = {New York University Law Review},
	volume = {80},
	number = {1},
	year = {2005},
	month = {April},
	pages = {1{\textendash}116},
	abstract = {Does the U.S. Supreme Court curtail rights and liberties when the nation{\textquoteright}s security is under threat? In hundreds of articles and books, and with renewed fervor since September 11, 2001, members of the legal community have warred over this question. Yet, not a single large-scale, quantitative study exists on the subject. Using the best data available on the causes and outcomes of every civil rights and liberties case decided by the Supreme Court over the past six decades and employing methods chosen and tuned especially for this problem, our analyses demonstrate that when crises threaten the nation{\textquoteright}s security, the justices are substantially more likely to curtail rights and liberties than when peace prevails. Yet paradoxically, and in contradiction to virtually every theory of crisis jurisprudence, war appears to affect only cases that are unrelated to the war. For these cases, the effect of war and other international crises is so substantial, persistent, and consistent that it may surprise even those commentators who long have argued that the Court rallies around the flag in times of crisis. On the other hand, we find no evidence that cases most directly related to the war are affected. We attempt to explain this seemingly paradoxical evidence with one unifying conjecture: Instead of balancing rights and security in high stakes cases directly related to the war, the Justices retreat to ensuring the institutional checks of the democratic branches. Since rights-oriented and process-oriented dimensions seem to operate in different domains and at different times, and often suggest different outcomes, the predictive factors that work for cases unrelated to the war fail for cases related to the war. If this conjecture is correct, federal judges should consider giving less weight to legal principles outside of wartime but established during wartime, and attorneys should see it as their responsibility to distinguish cases along these lines.},
	url = {http://gking.harvard.edu/files/abs/crisis-abs.shtml},
	author = {Lee Epstein and Daniel E. Ho and Gary King and Jeffrey A. Segal}
}
@article {StoKinZen05,
	title = {WhatIf: Software for Evaluating Counterfactuals},
	journal = {Journal of Statistical Software},
	volume = {15},
	number = {4},
	year = {2005},
	url = {http://gking.harvard.edu/whatif},
	author = {Heather Stoll and Gary King and Langchee Zeng}
}
@article {EpsHoKin05b,
	title = {Replication data for: The Supreme Court During Crisis: How War Affects Only Nonwar Cases},
	year = {2005},
	note = {{\underline{hdl:1902.1/RESUDVYWPE} UNF:3:ZmbzFbfqogNM0Gb6CcV52A== Murray Research Archive [Distributor]}},
	author = {Lee Epstein and Daniel E. Ho and Gary King and Jeffrey A. Segal}
}
@article {ImaKin04,
	title = {Did Illegal Overseas Absentee Ballots Decide the 2000 U.S. Presidential Election?},
	journal = {Perspectives on Politics},
	volume = {2},
	number = {3},
	year = {2004},
	month = {September},
	pages = {537{\textendash}549},
	abstract = {Although not widely known until much later, Al Gore received 202 more votes than George W. Bush on election day in Florida. George W. Bush is president because he overcame his election day deficit with overseas absentee ballots that arrived and were counted after election day. In the final official tally, Bush received 537 more votes than Gore. These numbers are taken from the official results released by the Florida Secretary of State{\textquoteright}s office and so do not reflect overvotes, undervotes, unsuccessful litigation, butterfly ballot problems, recounts that might have been allowed but were not, or any other hypothetical divergence between voter preferences and counted votes. After the election, the New York Times conducted a six month long investigation and found that 680 of the overseas absentee ballots were illegally counted, and no partisan, pundit, or academic has publicly disagreed with their assessment. In this paper, we describe the statistical procedures we developed and implemented for the Times to ascertain whether disqualifying these 680 ballots would have changed the outcome of the election. The methods involve adding formal Bayesian model averaging procedures to King{\textquoteright}s (1997) ecological inference model. Formal Bayesian model averaging has not been used in political science but is especially useful when substantive conclusions depend heavily on apparently minor but indefensible model choices, when model generalization is not feasible, and when potential critics are more partisan than academic. We show how we derived the results for the Times so that other scholars can use these methods to make ecological inferences for other purposes. We also present a variety of new empirical results that delineate the precise conditions under which Al Gore would have been elected president, and offer new evidence of the striking effectiveness of the Republican effort to convince local election officials to count invalid ballots in Bush counties and not count them in Gore counties.},
	url = {http://gking.harvard.edu/files/abs/ballots-abs.shtml},
	author = {Kosuke Imai and Gary King}
}
@article {BecKinZen04,
	title = {Theory and Evidence in International Conflict: Response to de Marchi, Gelpi, and Grynaviski},
	volume = {98},
	number = {2},
	year = {2004},
	month = {May},
	pages = {379-389},
	abstract = {We thank Scott de Marchi, Christopher Gelpi, and Jeffrey Grynaviski (2003; hereinafter dGG) for their careful attention to our work (Beck, King, and Zeng, 2000; hereinafter BKZ) and for raising some important methodological issues that we agree deserve readers{\textquoteright} attention. We are pleased that dGG{\textquoteright}s analyses are consistent with the theoretical conjecture about international conflict put forward in BKZ --- "The causes of conflict, theorized to be important but often found to be small or ephemeral, are indeed tiny for the vast majority of dyads, but they are large stable and replicable whenever the ex ante probability of conflict is large" (BKZ, p.21) --- and that dGG agree with our main methodological point that out-of-sample forecasting performance should always be one of the standards used to judge studies of international conflict, and indeed most other areas of political science. However, dGG frequently err when they draw methodological conclusions. Their central claim involves the superiority of logit over neural network models for international conflict data, as judged by forecasting performance and other properties such as ease of use and interpretation ("neural networks hold few unambiguous advantages... and carry significant costs" relative to logit; dGG, p.14). We show here that this claim, which would be regarded as stunning in any of the diverse fields in which both methods are more commonly used, is false. We also show that dGG{\textquoteright}s methodological errors and the restrictive model they favor cause them to miss and mischaracterize crucial patterns in the causes of international conflict. We begin in the next section by summarizing the growing support for our conjecture about international conflict. The second section discusses the theoretical reasons why neural networks dominate logistic regression, correcting a number of methodological errors. The third section then demonstrates empirically, in the same data as used in BKZ and dGG, that neural networks substantially outperform dGG{\textquoteright}s logit model. We show that neural networks improve on the forecasts from logit as much as logit improves on a model with no theoretical variables. We also show how dGG{\textquoteright}s logit analysis assumed, rather than estimated, the answer to the central question about the literature{\textquoteright}s most important finding, the effect of democracy on war. Since this and other substantive assumptions underlying their logit model are wrong, their substantive conclusion about the democratic peace is also wrong. The neural network models we used in BKZ not only avoid these difficulties, but they, or one of the other methods available that do not make highly restrictive assumptions about the exact functional form, are just what is called for to study the observable implications of our conjecture.},
	url = {http://gking.harvard.edu/files/abs/toe-resp-abs.shtml},
	author = {Nathaniel Beck and Gary King and Langche Zeng}
}
@article {KinMurSal04,
	title = {Enhancing the Validity and Cross-cultural Comparability of Measurement in Survey Research},
	journal = {American Political Science Review},
	volume = {98},
	number = {1},
	year = {2004},
	month = {February},
	pages = {191{\textendash}207},
	abstract = {We address two long-standing survey research problems: measuring complicated concepts, such as political freedom or efficacy, that researchers define best with reference to examples; and what to do when respondents interpret identical questions in different ways. Scholars have long addressed these problems with approaches to reduce incomparability, such as writing more concrete questions -- with uneven success. Our alternative is to measure directly response category incomparability and to correct for it. We measure incomparability via respondents{\textquoteright} assessments, on the same scale as the self-assessments to be corrected, of hypothetical individuals described in short vignettes. Since actual levels of the vignettes are invariant over respondents, variability in vignette answers reveals incomparability. Our corrections require either simple recodes or a statistical model designed to save survey administration costs. With analysis, simulations, and cross-national surveys, we show how response incomparability can drastically mislead survey researchers and how our approach can fix them.},
	url = {http://gking.harvard.edu/files/abs/vign-abs.shtml},
	author = {Gary King and Christopher J.L. Murray and Joshua A. Salomon and Ajay Tandon}
}
@article {GilKin04,
	title = {What to do When Your Hessian is Not Invertible: Alternatives to Model Respecification in Nonlinear Estimation},
	journal = {Sociological Methods and Research},
	volume = {32},
	number = {1},
	year = {2004},
	month = {August},
	pages = {54-87},
	abstract = {What should a researcher do when statistical analysis software terminates before completion with a message that the Hessian is not invertable? The standard textbook advice is to respecify the model, but this is another way of saying that the researcher should change the question being asked. Obviously, however, computer programs should not be in the business of deciding what questions are worthy of study. Although noninvertable Hessians are sometimes signals of poorly posed questions, nonsensical models, or inappropriate estimators, they also frequently occur when information about the quantities of interest exists in the data, through the likelihood function. We explain the problem in some detail and lay out two preliminary proposals for ways of dealing with noninvertable Hessians without changing the question asked.},
	url = {http://gking.harvard.edu/files/abs/help-abs.shtml},
	author = {Jeff Gill and Gary King}
}
@book {KinRosTan04,
	title = {Ecological Inference: New Methodological Strategies},
	year = {2004},
	publisher = {Cambridge University Press},
	organization = {Cambridge University Press},
	address = {New York},
	url = {http://gking.harvard.edu/files/abs/ecinf04-abs.shtml},
	editor = {Gary King and Ori Rosen and Martin A. Tanner}
}
@inbook {KinZen04,
	title = {Encyclopedia of Biopharmaceutical Statistics},
	year = {2004},
	publisher = {Marcel Dekker},
	organization = {Marcel Dekker},
	edition = {2nd},
	chapter = {Inference in Case-Control Studies},
	address = {New York},
	abstract = {Classic (or "cumulative") case-control sampling designs do not admit inferences about quantities of interest other than risk ratios, and then only by making the rare events assumption. Probabilities, risk differences, and other quantities cannot be computed without knowledge of the population incidence fraction. Similarly, density (or "risk set") case-control sampling designs do not allow inferences about quantities other than the rate ratio. Rates, rate differences, cumulative rates, risks, and other quantities cannot be estimated unless auxiliary information about the underlying cohort such as the number of controls in each full risk set is available. Most scholars who have considered the issue recommend reporting more than just the relative risks and rates, but auxiliary population information needed to do this is not usually available. We address this problem by developing methods that allow valid inferences about all relevant quantities of interest from either type of case-control study when completely ignorant of or only partially knowledgeable about relevant auxiliary population information.},
	url = {http://gking.harvard.edu/files/abs/1s-enc-abs.shtml},
	author = {Gary King and Langche Zeng},
	editor = {Shein-Chung Chow}
}
@inbook {GelKatKin04,
	title = {Rethinking the Vote: The Politics and Prospects of American Electoreal Reform},
	year = {2004},
	pages = {75-88},
	publisher = {Oxford University Press},
	organization = {Oxford University Press},
	chapter = {Chapter 5, Empirically Evaluating the Electoral College},
	address = {New York},
	abstract = {The 2000 U.S. presidential election rekindled interest in possible electoral reform. While most of the popular and academic accounts focused on balloting irregularities in Florida, such as the now infamous "butterfly" ballot and mishandled absentee ballots, some also noted that this election marked only the fourth time in history that the candidate with a plurality of the popular vote did not also win the Electoral College. This "anti-democratic" outcome has fueled desire for reform or even outright elimination of the electoral college. We show that after appropriate statistical analysis of the available historical electoral data, there is little basis to argue for reforming the Electoral College. We first show that while the Electoral College may once have been biased against the Democrats, the current distribution of voters advantages neither party. Further, the electoral vote will differ from the popular vote only when the average vote shares of the two major candidates are extremely close to 50 percent. As for individual voting power, we show that while there has been much temporal variation in relative voting power over the last several decades, the voting power of individual citizens would not likely increase under a popular vote system of electing the president.},
	url = {http://gking.harvard.edu/files/abs/rethink-abs.shtml},
	author = {Andrew Gelman and Jonathan Katz and Gary King},
	editor = {Ann N. Crigler and Marion R. Just and Edward J. McCaffery}
}
@inbook {KinRosTan04b,
	title = {Ecological Inference: New Methodological Strategies},
	year = {2004},
	publisher = {Cambridge University Press},
	organization = {Cambridge University Press},
	chapter = {Information in Ecological Inference: An Introduction},
	address = {New York},
	author = {Gary King and Ori Rosen and Martin Tanner},
	editor = {Gary King and Ori Rosen and Martin Tanner}
}
@article {King04,
	title = {EI: A Program for Ecological Inference},
	journal = {Journal of Statistical Software},
	volume = {11},
	number = {7},
	year = {2004},
	author = {Gary King}
}
@article {King04b,
	title = {Finding New Information for Ecological Inference Models: A Comment on Jon Wakefield, "Ecological Inference in 2 X 2 Tables"},
	journal = {Journal of the Royal Statistical Society},
	volume = {167},
	number = {Series A},
	year = {2004},
	pages = {437},
	author = {Gary King}
}
@article {BecKinZen04b,
	title = {Replication data for: Gelpi and Grynaviski},
	year = {2004},
	note = {{\underline{hdl:1902.1/LAAYCJJGDS} UNF:3:N0bEAswAlPPVXCxPOZYyqw== Murray Research Archive [Distributor]}},
	author = {Nathaniel Beck and Gary King and Langche Zeng}
}
@article {AdoKin03,
	title = {Analyzing Second Stage Ecological Regressions},
	journal = {Political Analysis},
	volume = {11},
	number = {1},
	year = {2003},
	month = {Winter},
	pages = {65-76},
	author = {Christopher Adolph and Gary King}
}
@article {AdoKinHer03,
	title = {A Consensus on Second Stage Analyses in Ecological Inference Models},
	journal = {Political Analysis},
	volume = {11},
	number = {1},
	year = {2003},
	month = {Winter},
	pages = {86{\textendash}94},
	abstract = {Since Herron and Shotts (2003a; hereinafter HS), Adolph and King (2003;hereinafter AK), and Herron and Shotts (2003b; hereinafter HS2), the four of us have iterated many more times, learned a great deal, and arrived at a consensus on this issue. This paper describes our joint recommendations for how to run second-stage ecological regressions, and provides detailed analyses to back up our claims.},
	url = {http://gking.harvard.edu/files/abs/akhs-abs.shtml},
	author = {Christopher Adolph and Gary King, with Michael C. Herron and Kenneth W. Shotts}
}
@article {KinLow03,
	title = {An Automated Information Extraction Tool For International Conflict Data with Performance as Good as Human Coders: A Rare Events Evaluation Design},
	journal = {International Organization},
	volume = {57},
	number = {3},
	year = {2003},
	month = {July},
	pages = {617-642},
	abstract = {Despite widespread recognition that aggregated summary statistics on international conflict and cooperation miss most of the complex interactions among nations, the vast majority of scholars continue to employ annual, quarterly, or occasionally monthly observations. Daily events data, coded from some of the huge volume of news stories produced by journalists, have not been used much for the last two decades. We offer some reason to change this practice, which we feel should lead to considerably increased use of these data. We address advances in event categorization schemes and software programs that automatically produce data by "reading" news stories without human coders. We design a method that makes it feasible for the first time to evaluate these programs when they are applied in areas with the particular characteristics of international conflict and cooperation data, namely event categories with highly unequal prevalences, and where rare events (such as highly conflictual actions) are of special interest. We use this rare events design to evaluate one existing program, and find it to be as good as trained human coders, but obviously far less expensive to use. For large scale data collections, the program dominates human coding. Our new evaluative method should be of use in international relations, as well as more generally in the field of computational linguistics, for evaluating other automated information extraction tools. We believe that the data created by programs similar to the one we evaluated should see dramatically increased use in international relations research. To facilitate this process, we are releasing with this article data on 4.3 million international events, covering the entire world for the last decade.},
	url = {http://gking.harvard.edu/files/abs/infoex-abs.shtml},
	author = {Gary King and Will Lowe}
}
@article {King03,
	title = {The Future of Replication},
	journal = {International Studies Perspectives},
	volume = {4},
	number = {1},
	year = {2003},
	month = {February},
	pages = {443{\textendash}499},
	abstract = {Since the replication standard was proposed for political science research, more journals have required or encouraged authors to make data available, and more authors have shared their data. The calls for continuing this trend are more persistent than ever, and the agreement among journal editors in this Symposium continues this trend. In this article, I offer a vision of a possible future of the replication movement. The plan is to implement this vision via the Virtual Data Center project, which -- by automating the process of finding, sharing, archiving, subsetting, converting, analyzing, and distributing data -- may greatly facilitate adherence to the replication standard.},
	url = {http://gking.harvard.edu/files/abs/replvdc-abs.shtml},
	author = {Gary King}
}
@article {EpsKin03,
	title = {Building An Infrastructure for Empirical Research in the Law [with comments from four law school deans]},
	journal = {Journal of Legal Education},
	volume = {53},
	number = {311},
	year = {2003},
	pages = {311{\textendash}320},
	abstract = {In every discipline in which "empirical research" has become commonplace, scholars have formed a subfield devoted to solving the methodological problems unique to that discipline{\textquoteright}s data and theoretical questions. Although students of economics, political science, psychology, sociology, business, education, medicine, public health, and so on primarily focus on specific substantive questions, they cannot wait for those in other fields to solve their methoodological problems or to teach them "new" methods, wherever they were initially developed. In "The Rules of Inference," we argued for the creation of an analogous methodological subfield devoted to legal scholarship. We also had two other objectives: (1) to adapt the rules of inference used in the natural and social sciences, which apply equally to quantitative and qualitative research, to the special needs, theories, and data in legal scholarship, and (2) to offer recommendations on how the infrastructure of teaching and research at law schools might be reorganized so that it could better support the creation of first-rate quantitative and qualitative empirical research without compromising other important objectives. Published commentaries on our paper, along with citations to it, have focused largely on the first-our application of the rules of inference to legal scholarship. Until now, discussions of our second goal-suggestions for the improvement of legal scholarship, as well as our argument for the creation of a group that would focus on methodological problems unique to law-have been relegated to less public forums, even though, judging from the volume of correspondence we have received, they seem to be no less extensive.},
	url = {http://gking.harvard.edu/files/abs/infra-abs.shtml},
	author = {Lee Epstein and Gary King}
}
@article {TomKinZen03,
	title = {ReLogit: Rare Events Logistic Regression},
	journal = {Journal of Statistical Software},
	volume = {8},
	number = {2},
	year = {2003},
	author = {Michael Tomz and Gary King and Langche Zeng}
}
@article {King03b,
	title = {10 Million International Dyadic Events},
	year = {2003},
	note = {{\underline{hdl:1902.1/FYXLAWZRIA} UNF:3:um06qkr/1tAwpS4roUqAiw== Murray Research Archive [Distributor]}},
	author = {Gary King}
}
@article {TomWitKin03,
	title = {CLARIFY: Software for Interpreting and Presenting Statistical Results},
	journal = {Journal of Statistical Software},
	volume = {8},
	number = {1},
	year = {2003},
	author = {Michael Tomz and Jason Wittenberg and Gary King}
}
@inbook {GakKin03,
	title = {Health Systems Performance Assessment: Debates, Methods and Empiricism},
	year = {2003},
	pages = {497-502},
	publisher = {World Health Organization},
	organization = {World Health Organization},
	chapter = {Chapter 36, Determinants of Inequality in Child Survival: Results from 39 Countries},
	address = {Geneva},
	author = {Emmanuela Gakidou and Gary King},
	editor = {Chrisopher Murray and David B. Evans}
}
@inbook {KinZen03,
	title = {Inference in Case-Control Studies},
	booktitle = {Encyclopedia of Biopharmaceutical Statistics},
	volume = {2nd edition},
	year = {2003},
	publisher = {Marcel Dekker},
	organization = {Marcel Dekker},
	address = {New York},
	author = {Gary King and Langche Zeng},
	editor = {Shein-Chung Chow, ed.}
}
@inbook {GilKin03,
	title = {Numerical Issues in Statistical Computing for the Social Scientist},
	year = {2003},
	pages = {143-176},
	publisher = {John Wiley and Sons, Inc.},
	organization = {John Wiley and Sons, Inc.},
	chapter = {Chapter 6, Numerical Issues Involved in Inverting Hessian Matrices},
	address = {Hoboken, NJ},
	author = {Jeff Gill and Gary King},
	editor = {Micah Altman and Jeff Gill and Michael P. McDonald}
}
@article {LowKin03,
	title = {Some Statistical Methods for Evaluating Information Extraction Systems},
	journal = {Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics},
	year = {2003},
	pages = {19-26},
	author = {Will Lowe and Gary King}
}
@article {EpsKin02b,
	title = {Empirical Research and The Goals of Legal Scholarship: A Response},
	journal = {University of Chicago Law Review},
	volume = {69},
	number = {1},
	year = {2002},
	month = {Winter},
	pages = {1{\textendash}209},
	abstract = {Although the term "empirical research" has become commonplace in legal scholarship over the past two decades, law professors have, in fact, been conducting research that is empirical -- that is, learning about the world using quantitative data or qualitative information -- for almost as long as they have been conducting research. For just as long, however, they have been proceeding with little awareness of, much less compliance with, the rules of inference, and without paying heed to the key lessons of the revolution in empirical analysis that has been taking place over the last century in other disciplines. The tradition of including some articles devoted to exclusively to the methododology of empirical analysis -- so well represented in journals in traditional academic fields -- is virtually nonexistent in the nation{\textquoteright}s law reviews. As a result, readers learn considerably less accurate information about the empirical world than the studies{\textquoteright} stridently stated, but overconfident, conclusions suggest. To remedy this situation both for the producers and consumers of empirical work, this Article adapts the rules of inference used in the natural and social sciences to the special needs, theories, and data in legal scholarship, and explicate them with extensive illustrations from existing research. The Article also offers suggestions for how the infrastructure of teaching and research at law schools might be reorganized so that it can better support the creation of first-rate empirical research without compromising other important objectives.},
	author = {Lee Epstein and Gary King}
}
@article {HonKinKat02,
	title = {A Fast, Easy, and Efficient Estimator for Multiparty Electoral Data},
	journal = {Political Analysis},
	volume = {10},
	number = {1},
	year = {2002},
	month = {Winter},
	pages = {84{\textendash}100},
	abstract = {Katz and King (1999) develop a model for predicting or explaining aggregate electoral results in multiparty democracies. This model is, in principle, analogous to what least squares regression provides American politics researchers in that two-party system. Katz and King applied this model to three-party elections in England and revealed a variety of new features of incumbency advantage and where each party pulls support from. Although the mathematics of their statistical model covers any number of political parties, it is computationally very demanding, and hence slow and numerically imprecise, with more than three. The original goal of our work was to produce an approximate method that works quicker in practice with many parties without making too many theoretical compromises. As it turns out, the method we offer here improves on Katz and King{\textquoteright}s (in bias, variance, numerical stability, and computational speed) even when the latter is computationally feasible. We also offer easy-to-use software that implements our suggestions.},
	url = {http://gking.harvard.edu/files/abs/trip-abs.shtml},
	author = {James Honaker and Gary King and Jonathan N. Katz}
}
@article {KinMur02,
	title = {Rethinking Human Security},
	journal = {Political Science Quarterly},
	volume = {116},
	number = {4},
	year = {2002},
	month = {Winter},
	pages = {585{\textendash}610},
	abstract = {In the last two decades, the international community has begun to conclude that attempts to ensure the territorial security of nation-states through military power have failed to improve the human condition. Despite astronomical levels of military spending, deaths due to military conflict have not declined. Moreover, even when the borders of some states are secure from foreign threats, the people within those states do not necessarily have freedom from crime, enough food, proper health care, education, or political freedom. In response to these developments, the international community has gradually moved to combine economic development with military security and other basic human rights to form a new concept of "human security". Unfortunately, by common assent the concept lacks both a clear definition, consistent with the aims of the international community, and any agreed upon measure of it. In this paper, we propose a simple, rigorous, and measurable definition of human security: the expected number of years of future life spent outside the state of "generalized poverty". Generalized poverty occurs when an individual falls below the threshold in any key domain of human well-being. We consider improvements in data collection and methods of forecasting that are necessary to measure human security and then introduce an agenda for research and action to enhance human security that follows logically in the areas of risk assessment, prevention, protection, and compensation.},
	url = {http://gking.harvard.edu/files/abs/hs-abs.shtml},
	author = {Gary King and Christopher J.L. Murray}
}
@article {EpsKin02,
	title = {The Rules of Inference},
	journal = {University of Chicago Law Review},
	volume = {69},
	number = {1},
	year = {2002},
	month = {Winter},
	pages = {1{\textendash}209},
	abstract = {Although the term "empirical research" has become commonplace in legal scholarship over the past two decades, law professors have, in fact, been conducting research that is empirical -- that is, learning about the world using quantitative data or qualitative information -- for almost as long as they have been conducting research. For just as long, however, they have been proceeding with little awareness of, much less compliance with, the rules of inference, and without paying heed to the key lessons of the revolution in empirical analysis that has been taking place over the last century in other disciplines. The tradition of including some articles devoted to exclusively to the methododology of empirical analysis -- so well represented in journals in traditional academic fields -- is virtually nonexistent in the nation{\textquoteright}s law reviews. As a result, readers learn considerably less accurate information about the empirical world than the studies{\textquoteright} stridently stated, but overconfident, conclusions suggest. To remedy this situation both for the producers and consumers of empirical work, this Article adapts the rules of inference used in the natural and social sciences to the special needs, theories, and data in legal scholarship, and explicate them with extensive illustrations from existing research. The Article also offers suggestions for how the infrastructure of teaching and research at law schools might be reorganized so that it can better support the creation of first-rate empirical research without compromising other important objectives.},
	url = {http://gking.harvard.edu/files/abs/rules-abs.shtml},
	author = {Lee Epstein and Gary King}
}
@article {King02b,
	title = {Isolating Spatial Autocorrelation, Aggregation Bias, and Distributional Violations in Ecological Inference},
	journal = {Political Analysis},
	volume = {10},
	number = {3},
	year = {2002},
	month = {Summer},
	pages = {298{\textendash}300},
	abstract = {This is an invited response to an article by Anselin and Cho. I make two main points: The numerical results in this article violate no conclusions from prior literature, and the absence of the deterministic information from the bounds in the article{\textquoteright}s analyses invalidates its theoretical discussion of spatial autocorrelation and all of its actual simulation results. An appendix shows how to draw simulations correctly.},
	url = {http://gking.harvard.edu/files/abs/ac-abs.shtml},
	author = {Gary King}
}
@article {KinZen02,
	title = {Improving Forecasts of State Failure},
	journal = {World Politics},
	volume = {53},
	number = {4},
	year = {2002},
	month = {July},
	pages = {623{\textendash}658},
	abstract = {We offer the first independent scholarly evaluation of the claims, forecasts, and causal inferences of the State Failure Task Force and their efforts to forecast when states will fail. State failure refers to the collapse of the authority of the central government to impose order, as in civil wars, revolutionary wars, genocides, politicides, and adverse or disruptive regime transitions. This task force, set up at the behest of Vice President Gore in 1994, has been led by a group of distinguished academics working as consultants to the U.S. Central Intelligence Agency. State Failure Task Force reports and publications have received attention in the media, in academia, and from public policy decision-makers. In this article, we identify several methodological errors in the task force work that cause their reported forecast probabilities of conflict to be too large, their causal inferences to be biased in unpredictable directions, and their claims of forecasting performance to be exaggerated. However, we also find that the task force has amassed the best and most carefully collected data on state failure in existence, and the required corrections which we provide, although very large in effect, are easy to implement. We also reanalyze their data with better statistical procedures and demonstrate how to improve forecasting performance to levels significantly greater than even corrected versions of their models. Although still a highly uncertain endeavor, we are as a consequence able to offer the first accurate forecasts of state failure, along with procedures and results that may be of practical use in informing foreign policy decision making. We also describe a number of strong empirical regularities that may help in ascertaining the causes of state failure.},
	url = {http://gking.harvard.edu/files/abs/civil-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {MurKinLop02,
	title = {Armed Conflict as a Public Health Problem},
	journal = {BMJ (British Medical Journal)},
	volume = {324},
	year = {2002},
	month = {February 9},
	pages = {346{\textendash}349},
	abstract = {Armed conflict is a major cause of injury and death worldwide, but we need much better methods of quantification before we can accurately assess its effect.

Armed conflict between warring states and groups within states have been major causes of ill health and mortality for most of human history. Conflict obviously causes deaths and injuries on the battlefield, but also health consequences from the displacement of populations, the breakdown of health and social services, and the heightened risk of disease transmission. Despite the size of the health consequences, military conflict has not received the same attention from public health research and policy as many other causes of illness and death. In contrast, political scientists have long studied the causes of war but have primarily been interested in the decision of elite groups to go to war, not in human death and misery.

We review the limited knowledge on the health consequences of conflict, suggest ways to improve measurement, and discuss the potential for risk assessment and for preventing and ameliorating the consequences of conflict.},
	url = {http://gking.harvard.edu/files/abs/armedph-abs.shtml},
	author = {Christopher J.L. Murray and Gary King and Alan D. Lopez and Niels Tomijima and Etienne Krug}
}
@article {GakKin02,
	title = {Measuring Total Health Inequality: Adding Individual Variation to Group-Level Differences},
	journal = {BioMed Central: International Journal for Equity in Health},
	volume = {1},
	number = {3},
	year = {2002},
	month = {August},
	abstract = {Background: Studies have revealed large variations in average health status across social, economic, and other groups. No study exists on the distribution of the risk of ill-health across individuals, either within groups or across all people in a society, and as such a crucial piece of total health inequality has been overlooked. Some of the reason for this neglect has been that the risk of death, which forms the basis for most measures, is impossible to observe directly and difficult to estimate. Methods: We develop a measure of total health inequality -- encompassing all inequalities among people in a society, including variation between and within groups -- by adapting a beta-binomial regression model. We apply it to children under age two in 50 low- and middle-income countries. Our method has been adopted by the World Health Organization and is being implemented in surveys around the world; preliminary estimates have appeared in the World Health Report (2000). Results: Countries with similar average child mortality differ considerably in total health inequality. Liberia and Mozambique have the largest inequalities in child survival, while Colombia, the Philippines and Kazakhstan have the lowest levels among the countries measured. Conclusions: Total health inequality estimates should be routinely reported alongside average levels of health in populations and groups, as they reveal important policy-related information not otherwise knowable. This approach enables meaningful comparisons of inequality across countries and future analyses of the determinants of inequality.},
	url = {http://gking.harvard.edu/files/abs/ebb-abs.shtml},
	author = {Emmanuela Gakidou and Gary King}
}
@article {KinZen02b,
	title = {Estimating Risk and Rate Levels, Ratios, and Differences in Case-Control Studies},
	journal = {Statistics in Medicine},
	volume = {21},
	year = {2002},
	pages = {1409{\textendash}1427},
	abstract = {Classic (or "cumulative") case-control sampling designs do not admit inferences about quantities of interest other than risk ratios, and then only by making the rare events assumption. Probabilities, risk differences, and other quantities cannot be computed without knowledge of the population incidence fraction. Similarly, density (or "risk set") case-control sampling designs do not allow inferences about quantities other than the rate ratio. Rates, rate differences, cumulative rates, risks, and other quantities cannot be estimated unless auxiliary information about the underlying cohort such as the number of controls in each full risk set is available. Most scholars who have considered the issue recommend reporting more than just the relative risks and rates, but auxiliary population information needed to do this is not usually available. We address this problem by developing methods that allow valid inferences about all relevant quantities of interest from either type of case-control study when completely ignorant of or only partially knowledgeable about relevant auxiliary population information.},
	url = {http://gking.harvard.edu/files/abs/1s-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {AltKinSig01,
	title = {Aggregation Among Binary, Count, and Duration Models: Estimating the Same Quantities from Different Levels of Data},
	journal = {Political Analysis},
	volume = {9},
	number = {1},
	year = {2001},
	month = {Winter},
	pages = {21{\textendash}44},
	abstract = {Binary, count and duration data all code discrete events occurring at points in time. Although a single data generation process can produce all of these three data types, the statistical literature is not very helpful in providing methods to estimate parameters of the same process from each. In fact, only single theoretical process exists for which know statistical methods can estimate the same parameters - and it is generally used only for count and duration data. The result is that seemingly trivial decisions abut which level of data to use can have important consequences for substantive interpretations. We describe the theoretical event process for which results exist, based on time independence. We also derive a set of models for a time-dependent process and compare their predictions to those of a commonly used model. Any hope of understanding and avoiding the more serious problems of aggregation bias in events data is contingent on first deriving a much wider arsenal of statistical models and theoretical processes that are not constrained by the particular forms of data that happen to be available. We discuss these issues and suggest an agenda for political methodologists interested in this very large class of aggregation problems.},
	url = {http://gking.harvard.edu/files/abs/abcd-abs.shtml},
	author = {James E. Alt and Gary King and Curtis Signorino}
}
@article {AltAndDig01a,
	title = {A Digital Library for the Dissemination and Replication of Quantitative Social Science Research: The Virtual Data Center},
	journal = {Social Science Computer Review},
	volume = {19},
	number = {4},
	year = {2001},
	month = {Winter},
	pages = {458{\textendash}470},
	abstract = {The Virtual Data Center (VDC) software is an open-source, digital library system for quantitative data. We discuss what the software does, and how it provides an infrastructure for the management and dissemination of disturbed collections of quantitative data, and the replication of results derived from this data.},
	url = {http://gking.harvard.edu/files/abs/vdcwhitepaper-abs.shtml},
	author = {Micah Altman and Leonid Andreev and Mark Diggory and Gary King and Daniel L. Kiskis and Elizabeth Kolster and M. Krot and Sidney Verba}
}
@article {KinZen01b,
	title = {Explaining Rare Events in International Relations},
	journal = {International Organization},
	volume = {55},
	number = {3},
	year = {2001},
	month = {Summer},
	pages = {693{\textendash}715},
	abstract = {Some of the most important phenomena in international conflict are coded s "rare events data," binary dependent variables with dozens to thousands of times fewer events, such as wars, coups, etc., than "nonevents". Unfortunately, rare events data are difficult to explain and predict, a problem that seems to have at least two sources. First, and most importantly, the data collection strategies used in international conflict are grossly inefficient. The fear of collecting data with too few events has led to data collections with huge numbers of observations but relatively few, and poorly measured, explanatory variables. As it turns out, more efficient sampling designs exist for making valid inferences, such as sampling all available events (e.g., wars) and a tiny fraction of non-events (peace). This enables scholars to save as much as 99\% of their (non-fixed) data collection costs, or to collect much more meaningful explanatory variables. Second, logistic regression, and other commonly used statistical procedures, can underestimate the probability of rare events. We introduce some corrections that outperform existing methods and change the estimates of absolute and relative risks by as much as some estimated effects reported in the literature. We also provide easy-to-use methods and software that link these two results, enabling both types of corrections to work simultaneously.},
	url = {http://gking.harvard.edu/files/abs/baby0s-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {KinZen01,
	title = {Logistic Regression in Rare Events Data},
	journal = {Political Analysis},
	volume = {9},
	number = {2},
	year = {2001},
	month = {Spring},
	pages = {137{\textendash}163},
	abstract = {We study rare events data, binary dependent variables with dozens to thousands of times fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological infections) than zeros ("nonevents"). In many literatures, these variables have proven difficult to explain and predict, a problem that seems to have at least two sources. First, popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events. We recommend corrections that outperform existing methods and change the estimates of absolute and relative risks by as much as some estimated effects reported in the literature. Second, commonly used data collection strategies are grossly inefficient for rare events data. The fear of collecting data with too few events has led to data collections with huge numbers of observations but relatively few, and poorly measured, explanatory variables, such as in international conflict data with more than a quarter-million dyads, only a few of which are at war. As it turns out, more efficient sampling designs exist for making valid inferences, such as sampling all variable events (e.g., wars) and a tiny fraction of nonevents (peace). This enables scholars to save as much as 99\% of their (nonfixed) data collection costs or to collect much more meaningful explanatory variables. We provide methods that link these two results, enabling both types of corrections to work simultaneously, and software that implements the methods developed.},
	url = {http://gking.harvard.edu/files/abs/0s-abs.shtml},
	author = {Gary King and Langche Zeng}
}
@article {KinHonJos01,
	title = {Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation},
	journal = {American Political Science Review},
	volume = {95},
	number = {1},
	year = {2001},
	month = {March},
	pages = {49{\textendash}69},
	abstract = {We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that "multiple imputation" is a superior approach to the problem of missing data scattered through one{\textquoteright}s explanatory and dependent variables than the methods currently used in applied data analysis. The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise. We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data. This algorithm is considerably easier to use than the leading method recommended in statistics literature. We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples. Finally, we offer easy-to-use that implements our suggested methods. (Software: AMELIA)},
	url = {http://gking.harvard.edu/files/abs/evil-abs.shtml},
	author = {Gary King and James Honaker and Anne Joseph and Kenneth Scheve}
}
@article {King01,
	title = {Proper Nouns and Methodological Propriety: Pooling Dyads in International Relations Data},
	journal = {International Organization},
	volume = {55},
	number = {2},
	year = {2001},
	month = {Fall},
	pages = {497{\textendash}507},
	abstract = {The intellectual stakes at issue in this symposium are very high: Green, Kim, and Yoon (2000; hereinafter GKY) apply their proposed methodological prescriptions and conclude that they key findings in the field is wrong; democracy "has no effect on militarized disputes." GKY are mainly interested in convincing scholars about their methodological points and see themselves as having no stake in the resulting substantive conclusions. However, their methodological points are also high stakes claims: if correct, the vast majority of statistical analyses of military conflict ever conducted would be invalidated. GKY say they "make no attempt to break new ground statistically," but, as we will see, this both understates their methodological contribution to the field and misses some unique features of their application and data in international relations. On the ltter, GKY{\textquoteright}s critics are united: Oneal and Russett (2000) conclude that GKY{\textquoteright}s method "produces distorted results," and show even in GKY{\textquoteright}s framework how democracy{\textquoteright}s effect can be reinstated. Beck and Katz (2000) are even more unambiguous: "GKY{\textquoteright}s conclusion, in table 3, that variables such as democracy have no pacific impact, is simply nonsense...GKY{\textquoteright}s (methodological) proposal...is NEVER a good idea." My given task is to sort out and clarify these conflicting claims and counterclaims. The procedure I followed was to engage in extensive discussions with the participants that included joint reanalyses provoked by our discussions and passing computer program code (mostly with Monte Carlo simulations) back and forth to ensure we were all talking about the same methods and agreed with the factual results. I learned a great deal from this process and believe that the positions of the participants are now a lot closer than it may seem from their written statements. Indeed, I believe that all the participants now agree with what I have written here, even though they would each have different emphases (and although my believing there is agreement is not the same as there actually being agreement!).},
	url = {http://gking.harvard.edu/files/abs/pool-abs.shtml},
	author = {Gary King}
}
@article {RosJiaKin01,
	title = {Bayesian and Frequentist Inference for Ecological Inference: The $R \times C$ Case},
	journal = {Statistica Neerlandica},
	volume = {55},
	number = {2},
	year = {2001},
	pages = {134{\textendash}156},
	abstract = {In this paper we propose Bayesian and frequentist approaches to ecological inference, based on R x C contingency tables, including a covariate. The proposed Bayesian model extends the binomial-beta hierarchical model developed by King, Rosen and Tanner (1999) from the 2 x 2 case to the R x C case, the inferential procedure employs Markov chain Monte Carlo (MCMC) methods. As such the resulting MCMC analysis is rich but computationally intensive. The frequentist approach, based on first moments rather than on the entire likelihood, provides quick inference via nonlinear least-squares, while retaining good frequentist properties. The two approaches are illustrated with simulated data, as well as with real data on voting patterns in Weimar Germany. In the final section of the paper we provide an overview of a range of alternative inferential approaches which trade-off computational intensity for statistical efficiency.},
	url = {http://gking.harvard.edu/files/abs/rosen-abs.shtml},
	author = {Ori Rosen and Wenxin Jiang and Gary King and Martin A. Tanner}
}
@article {AltAndDig01b,
	title = {An Overview of the Virtual Data Center Project and Software},
	journal = {JCDL {\textquoteright}01: First Joint Conference on Digital Libraries},
	year = {2001},
	pages = {203-204},
	abstract = {In this paper, we present an overview of the Virtual Data Center (VDC) software, an open-source digital library system for the management and dissemination of distributed collections of quantitative data. (see http://TheData.org). The VDC functionality provides everything necessary to maintain and disseminate an individual collection of research studies, including facilities for the storage, archiving, cataloging, translation, and on-line analysis of a particular collection. Moreover, the system provides extensive support for distributed and federated collections including: location-independent naming of objects, distributed authentication and access control, federated metadata harvesting, remote repository caching, and distributed "virtual" collections of remote objects.},
	url = {http://gking.harvard.edu/files/abs/jcdl01-abs.shtml},
	author = {Micah Altman and Leonid Andreev and Mark Diggory and Gary King and Daniel L. Kiskis and Elizabeth Kolster and M. Krot and Sidney Verba}
}
@article {KinZen01c,
	title = {Replication data for: Explaining Rare Events in International Relations},
	year = {2001},
	note = {\underline{hdl:1902.1/OUCBSJKXIC} UNF:3:vyct3c8fMCdWOdp03NUhaA== Murray Research Archive [Distributor]},
	author = {Gary King and Langche Zeng}
}
@article {KinZen01d,
	title = {Replication data for: Improving Forecats of State Failure},
	year = {2001},
	note = {{\underline{hdl:1902.1/RPQIODIANR} UNF:3:CEsbEgPxbxExfYuh2NWwWQ== Murray Research Archive [Distributor]}},
	author = {Gary King and Langche Zeng}
}
@article {King00,
	title = {Geography, Statistics, and Ecological Inference},
	journal = {Annals of the Association of American Geographers},
	volume = {90},
	number = {3},
	year = {2000},
	month = {September},
	pages = {601{\textendash}606},
	abstract = {I am grateful for such thoughtful review from these three distinguished geographers. Fotheringham provides an excellent summary of the approach offered, including how it combines the two methods that have dominated applications (and methodological analysis) for nearly half a century-- the method of bounds (Duncan and Davis, 1953) and Goodman{\textquoteright}s (1953) least squares regression. Since Goodman{\textquoteright}s regression is the only method of ecological inference "widely used in Geography" (O{\textquoteright}Loughlin), adding information that is known to be true from the method of bounds (for each observation) would seem to have the chance to improve a lot of research in this field. The other addition that EI provides is estimates at the lowest level of geography available, making it possible to map results, instead of giving only single summary numbers for the entire geographic region. Whether one considers the combined method offered "the" solution (as some reviewers and commentators have portrayed it), "a" solution (as I tried to describe it), or, perhaps better and more simply, as an improved method of ecological inference, is not importatnt. The point is that more data are better, and this method incorporates more. I am gratified that all three reviewers seem to support these basic points. In this response, I clarify a few points, correct some misunderstandings, and present additional evidence. I conclude with some possible directions for future research.},
	url = {http://gking.harvard.edu/files/abs/geog-abs.shtml},
	author = {Gary King}
}
@article {BecKinZen00,
	title = {Improving Quantitative Studies of International Conflict},
	journal = {American Political Science Review},
	volume = {94},
	number = {1},
	year = {2000},
	month = {March},
	pages = {21{\textendash}36},
	abstract = {We address a well-known but infrequently discussed problem in the quantitative study of international conflict: Despite immense data collections, prestigious journals, and sophisticated analyses, empirical findings in the literature on international conflict are often unsatisfying. Many statistical results change from article to article and specification to specification. Accurate forecasts are nonexistant. In this article we offer a conjecture about one source of this problem: The causes of conflict, theorized to be important but often found to be small or ephemeral, are indeed tiny for the vast majority of dyads, but they are large, stable, and replicable wherever the ex ante probability of conflict is large. This simple idea has an unexpectedly rich array of observable implications, all consistent with the literature. We directly test our conjecture by formulating a statistical model that includes critical features. Our approach, a version of a "neural network" model, uncovers some interesting structural features of international conflict, and as one evaluative measure, forecasts substantially better than any previous effort. Moreover, this improvement comes at little cost, and it is easy to evaluate whether the model is a statistical improvement over the simpler models commonly used.},
	url = {http://gking.harvard.edu/files/abs/improv-abs.shtml},
	author = {Nathaniel Beck and Gary King and Langche Zeng}
}
@article {KinTomWit00,
	title = {Making the Most of Statistical Analyses: Improving Interpretation and Presentation},
	journal = {American Journal of Political Science},
	volume = {44},
	number = {2},
	year = {2000},
	note = {{http://gking.harvard.edu/files/abs/making-abs.shtml}},
	month = {April},
	pages = {341{\textendash}355},
	abstract = {Social Scientists rarely take full advantage of the information available in their statistical results. As a consequence, they miss opportunities to present quantities that are of greatest substantive interest for their research and express the appropriate degree of certainty about these quantities. In this article, we offer an approach, built on the technique of statistical simulation, to extract the currently overlooked information from any statistical method and to interpret and present it in a reader-friendly manner. Using this technique requires some expertise, which we try to provide herein, but its application should make the results of quantitative articles more informative and transparent. To illustrate our recommendations, we replicate the results of several published works, showing in each case how the authors{\textquoteright} own conclusions can be expressed more sharply and informatively, and, without changing any data or statistical assumptions, how our approach reveals important new information about the research questions at hand. We also offer very easy-to-use Clarify software that implements our suggestions.},
	author = {Gary King and Michael Tomz and Jason Wittenberg}
}
@article {BecKinZen00b,
	title = {Replication data for: Improving Quantitative Studies of International Conflict: A Conjecture},
	journal = {American Political Science Review},
	volume = {94},
	year = {2000},
	note = {{\underline{hdl:1902.1/SZKONDGOMF} UNF:3:rYRDzT8dCJ/BR7V9u8fObA== Murray Research Archive [Distributor]}},
	month = {03/2000},
	pages = {21-36},
	author = {Nathaniel Beck and Gary King and Langche Zeng}
}
@article {KinTomWit00b,
	title = {Replication data for: Making the Most of Statistical Analyses: Improving Interpretation and Presentation},
	year = {2000},
	note = {{\underline{hdl:1902.1/QTCABXZZRQ} UNF:3:1VaLflZ/LfB+AISX+hBm1w== Murray Research Archive [Distributor]}},
	author = {Gary King and Michael Tomz and Jason Wittenberg}
}
@article {GelKinLiu99,
	title = {Not Asked and Not Answered: Multiple Imputation for Multiple Surveys},
	journal = {Journal of the American Statistical Association},
	volume = {93},
	number = {433},
	year = {1999},
	month = {September},
	pages = {846{\textendash}857},
	abstract = {We present a method of analyzing a series of independent cross-sectional surveys in which some questions are not answered in some surveys and some respondents do not answer some of the questions posed. The method is also applicable to a single survey in which different questions are asked or different sampling methods are used in different strata or clusters. Our method involves multiply imputing the missing items and questions by adding to existing methods of imputation designed for single surveys a hierarchical regression model that allows covariates at the individual and survey levels. Information from survey weights is exploited by including in the analysis the variables on which the weights are based, and then reweighting individual responses (observed and imputed) to estimate population quantities. We also develop diagnostics for checking the fit of the imputation model based on comparing imputed data to nonimputed data. We illustrate with the example that motivated this project: a study of pre-election public opinion polls in which not all the questions of interest are asked in all the surveys, so that it is infeasible to impute within each survey separately.},
	url = {http://gking.harvard.edu/files/abs/not-abs.shtml},
	author = {Andrew Gelman and Gary King and Chuanhai Liu}
}
@article {GelKinLiu99b,
	title = {Rejoinder: Not Asked and Not Answered: Multiple Imputation for Multiple Surveys},
	journal = {Journal of the American Statistical Association},
	volume = {93},
	number = {433},
	year = {1999},
	month = {September},
	pages = {869{\textendash}874},
	abstract = {We present a method of analyzing a series of independent cross-sectional surveys in which some questions are not answered in some surveys and some respondents do not answer some of the questions posed. The method is also applicable to a single survey in which different questions are asked or different sampling methods are used in different strata or clusters. Our method involves multiply imputing the missing items and questions by adding to existing methods of imputation designed for single surveys a hierarchical regression model that allows covariates at the individual and survey levels. Information from survey weights is exploited by including in the analysis the variables on which the weights are based, and then reweighting individual responses (observed and imputed) to estimate population quantities. We also develop diagnostics for checking the fit of the imputation model based on comparing imputed data to nonimputed data. We illustrate with the example that motivated this project: a study of pre-election public opinion polls in which not all the questions of interest are asked in all the surveys, so that it is infeasible to impute within each survey separately.},
	author = {Andrew Gelman and Gary King and Chuanhai Liu}
}
@article {King99,
	title = {The Future of Ecological Inference Research: A Reply to Freedman et al.},
	journal = {Journal of the American Statistical Association},
	volume = {94},
	number = {445},
	year = {1999},
	month = {March},
	pages = {352-355},
	abstract = {I appreciate the editor{\textquoteright}s invitation to reply to Freedman et al.{\textquoteright}s (1998) review of "A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from Aggregate Data" (Princeton University Press.) I welcome this scholarly critique and JASA{\textquoteright}s decision to publish in this field. Ecological inference is a large and very important area for applications that is especially rich with open statistical questions. I hope this discussion stimulates much new scholarship. Freedman et al. raise several interesting issues, but also misrepresent or misunderstand the prior literature, my approach, and their own empirical analyses, and compound the problem, by refusing requests from me and the editor to make their data and software available for this note. Some clarification is thus in order.},
	url = {http://gking.harvard.edu/files/abs/reply-abs.shtml},
	author = {Gary King}
}
@article {KatKin99,
	title = {A Statistical Model for Multiparty Electoral Data},
	journal = {American Political Science Review},
	volume = {93},
	number = {1},
	year = {1999},
	month = {March},
	pages = {15{\textendash}32},
	abstract = {We propose a comprehensive statistical model for analyzing multiparty, district-level elections. This model, which provides a tool for comparative politics research analagous to that which regression analysis provides in the American two-party context, can be used to explain or predict how geographic distributions of electoral results depend upon economic conditions, neighborhood ethnic compositions, campaign spending, and other features of the election campaign or aggregate areas. We also provide new graphical representations for data exploration, model evaluation, and substantive interpretation. We illustrate the use of this model by attempting to resolve a controversy over the size of and trend in electoral advantage of incumbency in Britain. Contrary to previous analyses, all based on measures now known to be biased, we demonstrate that the advantage is small but meaningful, varies substantially across the parties, and is not growing. Finally, we show how to estimate the party from which each party{\textquoteright}s advantage is predominantly drawn.},
	url = {http://gking.harvard.edu/files/abs/multiparty-abs.shtml},
	author = {Jonathan Katz and Gary King}
}
@article {KinLav99,
	title = {Many Publications, but Still No Evidence},
	journal = {Electoral Studies},
	volume = {18},
	number = {4},
	year = {1999},
	month = {December},
	pages = {597{\textendash}598},
	abstract = {In 1990, Budge and Hofferbert (B\&H) claimed that they had found solid evidence that party platforms cause U.S. budgetary priorities, and thus concluded that mandate theory applies in the United States as strongly as it does elsewhere. The implications of this stunning conclusion would mean that virtually every observer of the American party system in this century has been wrong.

King and Laver (1993) reanalyzed B\&H{\textquoteright}s data and demonstrated in two ways that there exists no evidence for a causal relationship. First, accepting their entire statistical model, and correcting only an algebraic error (a mistake in how they computed their standard errors), we showed that their hypothesized relationship holds up in fewer than half the tests they reported. Second, we showed that their statistical model includes a slightly hidden but politically implausible assumption that a new party achieves every budgetary desire immediately upon taking office. We then specified a model without this unrealistic assumption and we found that the assumption was not supported, and that all evidence in the data for platforms causing government budgets evaporated. In their published response to our article, B\&H withdrew their key claim and said they were now (in 1993) merely interested in an association and not causation. That is how it was left in 1993{\textemdash}a perfectly amicable resolution as far as we were concerned{\textemdash}since we have no objection to the claim that there is a non-causal or chance association between any two variables. Of course, we see little reason to be interested in non-causal associations in this area any more than in the chance correlation that exists between the winner of the baseball World Series and the party winning the U.S. presidency. Since party mandate theory only makes sense as a causal theory, the conventional wisdom about America{\textquoteright}s porous, non-mandate party system stands.},
	url = {http://gking.harvard.edu/files/abs/manypub-abs.shtml},
	author = {Gary King and Michael Laver}
}
@article {KinRosTan99,
	title = {Binomial-Beta Hierarchical Models for Ecological Inference},
	journal = {Sociological Methods and Research},
	volume = {28},
	number = {1},
	year = {1999},
	month = {August},
	pages = {61{\textendash}90},
	abstract = {The authors develop binomial-beta hierarchical models for ecological inference using insights from the literature on hierarchical models based on Markov chain Monte Carlo algorithms and King{\textquoteright}s ecological inference model. The new approach reveals some features of the data that King{\textquoteright}s approach does not, can easily be generalized to more complicated problems such as general R x C tables, allows the data analyst to adjust for covariates, and provides a formal evaluation of the significance of the covariates. It may also be better suited to cases in which the observed aggregate cells are estimated from very few observations or have some forms of measurement error. This article also provides an example of a hierarchical model in which the statistical idea of "borrowing strength" is used not merely to increase the efficiency of the estimates but to enable the data analyst to obtain estimates.},
	url = {http://gking.harvard.edu/files/abs/binom-abs.shtml},
	author = {Gary King and Ori Rosen and Martin A. Tanner}
}
@article {LewKin99,
	title = {No Evidence on Directional vs. Proximity Voting},
	journal = {Political Analysis},
	volume = {8},
	number = {1},
	year = {1999},
	month = {August},
	pages = {21{\textendash}33},
	abstract = {The directional and proximity models offer dramatically different theories for how voters make decisions and fundamentally divergent views of the supposed microfoundations on which vast bodies of literature in theoretical rational choice and empirical political behavior have been built. We demonstrate here that the empirical tests in the large and growing body of literature on this subject amount to theoretical debates about which statistical assumption is right. The key statistical assumptions have not been empirically tested and, indeed, turn out to be effectively untestable with exiting methods and data. Unfortunately, these assumptions are also crucial since changing them leads to different conclusions about voter processes.},
	url = {http://gking.harvard.edu/files/abs/spatial-abs.shtml},
	author = {Jeffrey Lewis and Gary King}
}
@article {KatKin99b,
	title = {Replication data for: A Statistical Model of Multiparty Electoral Data},
	year = {1999},
	note = {{\underline{hdl:1902.1/QIGTWZYTLZ} UNF:3:gwGcKylle0BKJTGv3Zv4OA== Murray Research Archive [Distributor]}},
	author = {Jonathan Katz and Gary King}
}
@article {GelKinBos98,
	title = {Estimating the Probability of Events that Have Never Occurred: When Is Your Vote Decisive?},
	journal = {Journal of the American Statistical Association},
	volume = {93},
	number = {441},
	year = {1998},
	month = {March},
	pages = {1{\textendash}9},
	abstract = {Researchers sometimes argue that statisticians have little to contribute when few realizations of the process being estimated are observed. We show that this argument is incorrect even in the extreme situation of estimating the probabilities of events so rare that they have never occurred. We show how statistical forecasting models allow us to use empirical data to improve inferences about the probabilities of these events. Our application is estimating the probability that your vote will be decisive in a U.S. presidential election, a problem that has been studied by political scientists for more than two decades. The exact value of this probability is of only minor interest, but the number has important implications for understanding the optimal allocation of campaign resources, whether states and voter groups receive their fair share of attention from prospective presidents, and how formal "rational choice" models of voter behavior might be able to explain why people vote at all. We show how the probability of a decisive vote can be estimated empirically from state-level forecasts of the presidential election and illustrate with the example of 1992. Based on generalizations of standard political science forecasting models, we estimate the (prospective) probability of a single vote being decisive as about 1 in 10 million for close national elections such as 1992, varying by about a factor of 10 among states. Our results support the argument that subjective probabilities of many types are best obtained through empirically based statistical prediction models rather than solely through mathematical reasoning. We discuss the implications of our findings for the types of decision analyses used in public choice studies.},
	url = {http://gking.harvard.edu/files/abs/estimatprob-abs.shtml},
	author = {Andrew Gelman and Gary King and John Boscardin}
}
@article {KinPal98,
	title = {The Record of American Democracy, 1984-1990},
	journal = {Sociological Methods and Research},
	volume = {26},
	number = {3},
	year = {1998},
	month = {February},
	pages = {424{\textendash}427},
	url = {http://www.hmdc.harvard.edu/ROAD},
	author = {Gary King and Bradley Palmquist}
}
@article {HonJosKin98,
	title = {AMELIA: A Program for Missing Data},
	year = {1998},
	url = {http://gking.harvard.edu/amelia},
	author = {James Honaker and Anne Joseph and Gary King and Kenneth Scheve and Naunihal Singh.}
}
@article {King98,
	title = {MAXLIK, a set of Gauss programs, annotated for pedagogical purposes, to implement the maximum likelihood models in Unifying Political Methodology: The Likelihood Theory of Statistical Inference},
	year = {1998},
	author = {Gary King}
}
@article {GelKinBos98b,
	title = {Replication data for: Estimating the Probability of Events that have Never Occurred: When is your Vote Decisive},
	year = {1998},
	note = {{\underline{hdl:1902.1/NOLXXTUHNZ} UNF:3:ORDulVH6qEb4lsCyDn5W3A== Murray Research Archive [Distributor]}},
	author = {Andrew Gelman and Gary King and John Boscardin}
}
@book {King97,
	title = {A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from Aggregate Data},
	year = {1997},
	publisher = {Princeton University Press},
	organization = {Princeton University Press},
	address = {Princeton},
	url = {http://gking.harvard.edu/eicamera/kinroot.html},
	author = {Gary King}
}
@article {King97b,
	title = {Replication data for: A Solution to the Ecological Inference Problem: Reconstructing Individuals Behavior from Aggregate Data},
	year = {1997},
	note = {{\underline{hdl:1902.1/LWMMKUTYXS} UNF:3:DRWozWd89+vNLO7lY2AHbg== Murray Research Archive [Distributor]}},
	author = {Gary King}
}
@article {BenKin96,
	title = {A Preview of EI and EzI: Programs for Ecological Inference},
	journal = {Social Science Computer Review},
	volume = {14},
	number = {4},
	year = {1996},
	month = {Winter},
	pages = {433{\textendash}438},
	abstract = {Ecological inference, as traditionally defined, is the process of using aggregate (i.e., "ecological") data to infer discrete individual-level relationships of interest when individual-level data are not available. Existing methods of ecological inference generate very inaccurate conclusions about the empirical world- which thus gives rise to the ecological inference problem. Most scholars who analyze aggregate data routinely encounter some form of this problem. EI (by Gary King) and EzI (by Kenneth Benoit and Gary King) are freely available software that implement the statistical and graphical methods detailed in Gary King{\textquoteright}s book A Solution to the Ecological Inference Problem. These methods make it possible to infer the attributes of individual behavior from aggregate data. EI works within the statistics program Gauss and will run on any computer hardware and operating system that runs Gauss (the Gauss module, CML, or constrained maximum likelihood- by Ronald J. Schoenberg- is also required). EzI is a menu-oriented stand-alone version of the program that runs under MS-DOS (and soon Windows 95, OS/2, and HP-UNIX). EI allows users to make ecological inferences as part of the powerful and open Gauss statistical environment. In contrast, EzI requires no additional software, and provides an attractive menu-based user interface for non-Gauss users, although it lacks the flexibility afforded by the Gauss version. Both programs presume that the user has read or is familiar with A Solution to the Ecological Inference Problem.},
	url = {http://gking.harvard.edu/files/abs/preview-abs.shtml},
	author = {Kenneth Benoit and Gary King}
}
@article {King96,
	title = {Why Context Should Not Count},
	journal = {Political Geography},
	volume = {15},
	number = {2},
	year = {1996},
	month = {February},
	pages = {159{\textendash}164},
	abstract = {This paper is an invited comment on a paper by John Agnew. I largely agree with Agnew{\textquoteright}s comments and thus focus on remaining areas wehre an alternative perspective might be useful. My argument is that political geographers should not be so concerned with demonstrating that context matters. My reasoning is based on three arguments. First, in fact context rarely counts (Section 1) and, second, the most productive practical goal for political researchers should be to show that it does not count (Section 2). Finally, a disproportionate focus on {\textquoteleft}context counting{\textquoteright} can lead, and has led, to some seriosu problems in practical research situations, such as attempting to give theoretical answers to empirical questions (Section 3) and empirical answers to theoretical questions (Section 4).},
	url = {http://gking.harvard.edu/files/abs/contxt-abs.shtml},
	author = {Gary King}
}
@inbook {GelKin96,
	title = {Advantages of Conflictual Redistricting},
	booktitle = {Fixing the Boundary: Defining and Redefining Single-Member Electoral Districts},
	year = {1996},
	pages = {207{\textendash}218},
	publisher = {Dartmouth Publishing Company},
	organization = {Dartmouth Publishing Company},
	address = {Aldershot, England},
	abstract = {This article describes the results of an analysis we did of state legislative elections in the United States, where each state is required to redraw the boundaries of its state legislative districts every ten years. In the United States, redistrictings are sometimes controlled by the Democrats, sometimes by the Republicans, and sometimes by bipartisan committees, but never by neutral boundary commissions. Our goal was to study the consequences of redistricting; at the conclusion of this article, we discuss how our findings might be relevant to British elections.},
	url = {http://gking.harvard.edu/files/abs/advant-abs.shtml},
	author = {Andrew Gelman and Gary King},
	editor = {Iain McLean and David Butler, eds}
}
@article {King96b,
	title = {EI: Program for Ecological Inference},
	year = {1996},
	author = {Gary King}
}
@article {KinSig96,
	title = {The Generalization in the Generalized Event Count Model},
	journal = {Political Analysis},
	volume = {6},
	year = {1996},
	pages = {225{\textendash}252},
	abstract = {We use an analogy with the normal distribution and linear regression to demonstrate the need for the Generalize Event Count (GEC) model. We then show how the GEC provides a unified framework within which to understand a diversity of distributions used to model event counts, and how to express the model in one simple equation. Finally, we address the points made by Christopher Achen, Timothy Amato, and John Londregan. Amato{\textquoteright}s and Londregan{\textquoteright}s arguments are consistent with ours and provide additional interesting information and explanations. Unfortunately, the foundation on which Achen built his paper turns out to be incorrect, rendering all his novel claims about the GEC false (or in some cases irrelevant).},
	url = {http://gking.harvard.edu/files/abs/generaliz-abs.shtml},
	author = {Gary King and Curtis S. Signorino}
}
@inbook {KinBruGel96,
	title = {Racial Fairness in Legislative Redistricting},
	booktitle = {Classifying by Race},
	year = {1996},
	publisher = {Princeton University Press},
	organization = {Princeton University Press},
	abstract = {In this chapter, we study standards of racial fairness in legislative redistricting- a field that has been the subject of considerable legislation, jurisprudence, and advocacy, but very little serious academic scholarship. We attempt to elucidate how basic concepts about "color-blind" societies, and similar normative preferences, can generate specific practical standards for racial fairness in representation and redistricting. We also provide the normative and theoretical foundations on which concepts such as proportional representation rest, in order to give existing preferences of many in the literature a firmer analytical foundation.},
	url = {http://gking.harvard.edu/files/abs/racial-abs.shtml},
	author = {Gary King and John Bruce and Andrew Gelman},
	editor = {Paul E. Peterson, ed.}
}
@article {VosGelKin95,
	title = {Pre-Election Survey Methodology: Details From Nine Polling Organizations, 1988 and 1992},
	journal = {Public Opinion Quarterly},
	volume = {59},
	number = {1},
	year = {1995},
	month = {Spring},
	pages = {98{\textendash}132},
	abstract = {Before every presidential election, journalists, pollsters, and politicians commission dozens of public opinion polls. Although the primary function of these surveys is to forecast the election winners, they also generate a wealth of political data valuable even after the election. These preelection polls are useful because they are conducted with such frequency that they allow researchers to study change in estimates of voter opinion within very narrow time increments (Gelman and King 1993). Additionally, so many are conducted that the cumulative sample size of these polls is large enough to construct aggregate measures of public opinion within small demographic or geographical groupings (Wright, Erikson, and McIver 1985).},
	url = {http://gking.harvard.edu/files/abs/preelection-abs.shtml},
	author = {D. Steven Voss and Andrew Gelman and Gary King}
}
@article {King95,
	title = {Replication, Replication},
	journal = {PS: Political Science and Politics},
	volume = {28},
	number = {3},
	year = {1995},
	month = {September},
	pages = {443{\textendash}499},
	abstract = {Political science is a community enterprise; the community of empirical political scientists need access to the body of data necessary to replicate existing studies to understand, evaluate, and especially build on this work. Unfortunately, the norms we have in place now do not encourage, or in some cases even permit, this aim. Following are suggestions that would facilitate replication and are easy to implement -- by teachers, students, dissertation writers, graduate programs, authors, reviewers, funding agencies, and journal and book editors.},
	url = {http://gking.harvard.edu/files/abs/replication-abs.shtml},
	author = {Gary King}
}
@article {King95b,
	title = {A Revised Proposal, Proposal},
	journal = {PS: Political Science and Politics},
	volume = {XXVIII},
	number = {3},
	year = {1995},
	month = {September},
	pages = {494{\textendash}499},
	author = {Gary King}
}
@article {KinKeoVer95,
	title = {The Importance of Research Design in Political Science},
	journal = {American Political Science Review},
	volume = {89},
	number = {2},
	year = {1995},
	month = {June},
	pages = {454{\textendash}481},
	abstract = {Receiving five serious reviews in this symposium is gratifying and confirms our belief that research design should be a priority for our discipline. We are pleased that our five distinguished reviewers appear to agree with our unified approach to the logic of inference in the social sciences, and with our fundamental point: that good quantitative and good qualitative research designs are based fundamentally on the same logic of inference. The reviewers also raised virtually no objections to the main practical contribution of our book-- our many specific procedures for avoiding bias, getting the most out of qualitative data, and making reliable inferences.

However, the reviews make clear that although our book may be the latest word on research design in political science, it is surely not the last. We are taxed for failing to include important issues in our analysis and for dealing inadequately with some of what we included. Before responding to the reviewers{\textquoteright} more direct criticisms, let us explain what we emphasize in Designing Social Inquiry and how it relates to some of the points raised by the reviewers.},
	url = {http://gking.harvard.edu/files/abs/kkvresp-abs.shtml},
	author = {Gary King and Robert O. Keohane and Sidney Verba}
}
@article {WinSigKin95,
	title = {A Correction for an Underdispersed Event Count Probability Distribution},
	journal = {Political Analysis},
	year = {1995},
	pages = {215{\textendash}228},
	abstract = {We demonstrate that the expected value and variance commonly given for a well-known probability distribution are incorrect. We also provide corrected versions and report changes in a computer program to account for the known practical uses of this distribution.},
	url = {http://gking.harvard.edu/files/abs/correction-abs.shtml},
	author = {Rainer Winkelmann and Curtis Signorino and Gary King}
}
@article {GelKin94b,
	title = {Enhancing Democracy Through Legislative Redistricting},
	journal = {American Political Science Review},
	volume = {88},
	number = {3},
	year = {1994},
	month = {September},
	pages = {541{\textendash}559},
	abstract = {We demonstrate the surprising benefits of legislative redistricting (including partisan gerrymandering) for American representative democracy. In so doing, our analysis resolves two long-standing controversies in American politics. First, whereas some scholars believe that redistricting reduces electoral responsiveness by protecting incumbents, others, that the relationship is spurious, we demonstrate that both sides are wrong: redistricting increases responsiveness. Second, while some researchers believe that gerrymandering dramatically increases partisan bias and others deny this effect, we show both sides are in a sense correct. Gerrymandering biases electoral systems in favor of the party that controls the redistricting as compared to what would have happened if the other party controlled it, but any type of redistricting reduces partisan bias as compared to an electoral system without redistricting. Incorrect conclusions in both literatures resulted from misjudging the enormous uncertainties present during redistricting periods, making simplified assumptions about the redistricters{\textquoteright} goals, and using inferior statistical methods.},
	url = {http://gking.harvard.edu/files/abs/red-abs.shtml},
	author = {Andrew Gelman and Gary King}
}
@article {GelKin94,
	title = {A Unified Method of Evaluating Electoral Systems and Redistricting Plans},
	journal = {American Journal of Political Science},
	volume = {38},
	number = {2},
	year = {1994},
	month = {May},
	pages = {514{\textendash}554},
	abstract = {We derive a unified statistical method with which one can produce substantially improved definitions and estimates of almost any feature of two-party electoral systems that can be defined based on district vote shares. Our single method enables one to calculate more efficient estimates, with more trustworthy assessments of their uncertainty, than each of the separate multifarious existing measures of partisan bias, electoral responsiveness, seats-votes curves, expected or predicted vote in each district in a legislature, the probability that a given party will win the seat in each district, the proportion of incumbents or others who will lose their seats, the proportion of women or minority candidates to be elected, the incumbency advantage and other causal effects, the likely effects on the electoral system and district votes of proposed electoral reforms, such as term limitations, campaign spending limits, and drawing majority-minority districts, and numerous others. To illustrate, we estimate the partisan bias and electoral responsiveness of the U.S. House of Representatives since 1900 and evaluate the fairness of competing redistricting plans for the 1992 Ohio state legislature.},
	url = {http://gking.harvard.edu/files/abs/writeit-abs.shtml},
	author = {Andrew Gelman and Gary King}
}
@article {AltKin94,
	title = {Transfers of Governmental Power: The Meaning of Time Dependence},
	journal = {Comparative Political Studies},
	volume = {27},
	number = {2},
	year = {1994},
	month = {July},
	pages = {190{\textendash}210},
	abstract = {King, Alt, Burns, and Laver (1990) proposed and estimated a unified model in which cabinet durations depended on seven explanatory variables reflecting features of the cabinets and the bargaining environments in which they formed, along with a stochastic component in which the risk of a cabinet falling was treated as a constant across its tenure. Two recent research reports take issue with one aspect of this model. Warwick and Easton replicate the earlier findings for explanatory variables but claim that the stochastic risk should be seen as rising, and at a rate which varies, across the life of the cabinet. Bienen and van de Walle, using data on the duration of leaders, allege that random risk is falling. We continue in our goal of unifying this literature by providing further estimates with both cabinet and leader duration data that confirm the original explanatory variables{\textquoteright} effects, showing that leaders{\textquoteright} durations are affected by many of the same factors that affect the durability of the cabinets they lead, demonstrating that cabinets have stochastic risk of ending that is indeed constant across the theoretically most interesting range of durations, and suggesting that stochastic risk for leaders in countries with cabinet government is, if not constant, more likely to rise than fall.},
	url = {http://gking.harvard.edu/files/abs/transfers-abs.shtml},
	author = {James E. Alt and Gary King}
}
@book {KinKeoVer94,
	title = {Designing Social Inquiry: Scientific Inference in Qualitative Research},
	year = {1994},
	publisher = {Princeton University Press},
	organization = {Princeton University Press},
	address = {Princeton},
	url = {http://www.pupress.princeton.edu/titles/5458.html},
	author = {Gary King and Robert O. Keohane and Sidney Verba}
}
@inbook {GelKin94c,
	title = {Party Competition and Media Messages in U.S. Presidential Election Campaigns},
	booktitle = {The Parties Respond: Changes in the American Party System},
	year = {1994},
	pages = {255-295},
	publisher = {Westview Press},
	organization = {Westview Press},
	address = {Boulder, Colorado},
	abstract = {At one point during the 1988 campaign, Michael Dukakis was ahead in the public opinion polls by 17 percentage points, but he eventually lost the election by 8 percent. Walter Mondale was ahead in the polls by 4 percent during the 1984 campaign but lost the election in a landslide. During June and July of 1992, Clinton, Bush, and Perot each had turns in the public opinion poll lead. What explains all this poll variation? Why do so many citizens change their minds so quickly about presidential choices?},
	url = {http://gking.harvard.edu/files/abs/partycomp-abs.shtml},
	author = {Andrew Gelman and Gary King},
	editor = {L. Sandy Maisel}
}
@article {King94,
	title = {Elections to the United States House of Representatives, 1898-1992},
	year = {1994},
	note = {{\underline{hdl:1902.1/TQDSSPRDDZ} UNF:3:tD8SznMFjKIxWxOqTQaamQ== Murray Research Archive [Distributor]}},
	author = {Gary King}
}
@article {GelKin94d,
	title = {Replication data for: Enhancing Democracy Through Legislative Redistricting},
	year = {1994},
	note = {{\underline{hdl:1902.1/BNCOWNVERH} UNF:3:ZXahi7PBFxLRb46sVKOAuQ== Murray Research Archive [Distributor]}},
	author = {Andrew Gelman and Gary King}
}
@article {GelKin94e,
	title = {Replication data for: Unified Methods of Evaluating Electoral Systems and Redistricting Plans: United States House of Representatives adn Ohio State Legislature},
	year = {1994},
	note = {{\underline{hdl:1902.1/JWFTSFKOBK} UNF:3:Fi01DWj4Sx+0ZEOEo4TOXA== Murray Research Archive [Distributor]}},
	author = {Andrew Gelman and Gary King}
}
@article {KinWal93,
	title = {Good Research and Bad Research: Extending Zimile{\textquoteright}s Criticism},
	journal = {Early Childhood Research Quarterly},
	volume = {8},
	number = {3},
	year = {1993},
	month = {September},
	pages = {397{\textendash}401},
	abstract = {Herbert Zimiles has written a provocative article on quantitative research. Because his specific critiques of research on infant day care are nominal examples of his much broader arguments, we focus only on his general methodological perspectives in this brief comment. We write as methodologists, a qualitative researcher with a quantitative background (Walsh) and a quantitative researcher completing a book on qualitative research (King; see King, Keohane \& Verba, in preparation).},
	url = {http://gking.harvard.edu/files/abs/good-abs.shtml},
	author = {Gary King and Daniel J. Walsh}
}
@article {KinLav93,
	title = {On Party Platforms, Mandates, and Government Spending},
	journal = {American Political Science Review},
	volume = {87},
	number = {3},
	year = {1993},
	month = {September},
	pages = {744{\textendash}750},
	abstract = {In their 1990 Review article, Ian Budge and Richard Hofferbert analyzed the relationship between party platform emphases, control of the White House, and national government spending priorities, reporting strong evidence of a "party mandate" connection between them. Gary King and Michael Laver successfully replicate the original analysis, critique the interpretation of the causal effects, and present a reanalysis showing that platforms have small or nonexistent effects on spending. In response, Budge, Hofferbert, and Michael McDonald agree that their language was somewhat inconsistent on both interactions and causality but defend their conceptualization of "mandates" as involving only an association, not necessarily a causal connection, between party commitments and government policy. Hence, while the causes of government policy are of interest, noncausal associations are sufficient as evidence of party mandates in American politics.},
	url = {http://gking.harvard.edu/files/abs/hoff-abs.shtml},
	author = {Gary King and Michael Laver}
}
@article {GelKin93,
	title = {Why are American Presidential Election Campaign Polls so Variable when Votes are so Predictable?},
	journal = {British Journal of Political Science},
	volume = {23},
	number = {1},
	year = {1993},
	month = {October},
	pages = {409{\textendash}451},
	abstract = {As most political scientists know, the outcome of the U.S. Presidential election can be predicted within a few percentage points (in the popular vote), based on information available months before the election. Thus, the general election campaign for president seems irrelevant to the outcome (except in very close elections), despite all the media coverage of campaign strategy. However, it is also well known that the pre-election opinion polls can vary wildly over the campaign, and this variation is generally attributed to events in the campaign. How can campaign events affect people{\textquoteright}s opinions on whom they plan to vote for, and yet not affect the outcome of the election? For that matter, why do voters consistently increase their support for a candidate during his nominating convention, even though the conventions are almost entirely predictable events whose effects can be rationally forecast?

In this exploratory study, we consider several intuitively appealing, but ultimately wrong, resolutions to this puzzle, and discuss our current understanding of what causes opinion polls to fluctuate and yet reach a predictable outcome. Our evidence is based on graphical presentation and analysis of over 67,000 individual-level responses from forty-nine commercial polls during the 1988 campaign and many other aggregate poll results from the 1952--1992 campaigns.

We show that responses to pollsters during the campaign are not generally informed or even, in a sense we describe, "rational." In contrast, voters decide which candidate to eventually support based on their enlightened preferences, as formed by the information they have learned during the campaign, as well as basic political cues such as ideology and party identification. We cannot prove this conclusion, but we do show that it is consistent with the aggregate forecasts and individual-level opinion poll responses. Based on the enlightened preferences hypothesis, we conclude that the news media have an important effect on the outcome of Presidential elections---not due to misleading advertisements, sound bites, or spin doctors, but rather by conveying candidates{\textquoteright} positions on important issues.},
	url = {http://gking.harvard.edu/files/abs/variable-abs.shtml},
	author = {Andrew Gelman and Gary King}
}
@article {KingBruGil93,
	title = {The Science of Political Science Graduate Admissions},
	journal = {PS: Political Science and Politics},
	volume = {XXVI},
	number = {4},
	year = {1993},
	month = {December},
	pages = {772{\textendash}778},
	abstract = {As political scientists, we spend much time teaching and doing scholarly research, and more time than we may wish to remember on university committees. However, just as many of us believe that teaching and research are not fundamentally different activities, we also need not use fundamentally different standards of inference when studying government, policy, and politics than when participating in the governance of departments and universities. In this article, we describe our attempts to bring somewhat more systematic methods to the process and policies of graduate admissions.},
	url = {http://gking.harvard.edu/files/abs/admis-abs.shtml},
	author = {Gary King and John M. Bruce and Michael Gilligan}
}
@inbook {King93,
	title = {The Methodology of Presidential Research},
	booktitle = {Researching the Presidency: Vital Questions, New Approaches},
	year = {1993},
	pages = {387{\textendash}412},
	publisher = {University of Pittsburgh},
	organization = {University of Pittsburgh},
	address = {Pittsburgh},
	abstract = {The original purpose of the paper this chapter was based on was to use the Presidency Research Conference{\textquoteright}s first-round papers-- by John H. Aldrich, Erwin C. Hargrove, Karen M. Hult, Paul Light, and Richard Rose-- as my "data." My given task was to analyze the literature ably reviewed by these authors and report what political methodology might have to say about presidency research. I focus in this chapter on the traditional presidency literature, emphasizing research on the president and the office. For the most part, I do not consider research on presidential selection, election, and voting behavior, which has been much more similar to other fields in American politics.},
	url = {http://gking.harvard.edu/files/abs/methpres-abs.shtml},
	author = {Gary King},
	editor = {George Edwards, III, John H. Kessel, and Bert A. Rockman, eds.}
}
@article {KinLav93b,
	title = {Replication data for: On Party Platforms, Mandates, and Government Spending},
	year = {1993},
	note = {{\underline{hdl:1902.1/XEHYCJAWQD} UNF:3:cwNXuRQ/6Lp72obLkttmGg== Murray Research Archive [Distributor]}},
	author = {Gary King and Michael Laver}
}
@article {GelKin93b,
	title = {Replication data for: Why Are American Presidential Election Campaign Polls so Variable When Votes are so Predictable?},
	year = {1993},
	note = {{\underline{hdl:1902.1/SBBXEUSSCW} Murray Research Archive [Distributor]}},
	author = {Andrew Gelman and Gary King}
}
@article {GelKin92,
	title = {JudgeIt: A Program for Evaluating Electoral Systems and Redistricting Plans},
	year = {1992},
	author = {Andrew Gelman and Gary King}
}
@article {King91,
	title = {{\textquoteright}Truth{\textquoteright} is Stranger than Prediction, More Questionable Than Causal Inference},
	journal = {American Journal of Political Science},
	volume = {35},
	number = {4},
	year = {1991},
	month = {November},
	pages = {1047{\textendash}1053},
	abstract = {Robert Luskin{\textquoteright}s article in this issue provides a useful service by appropriately qualifying several points I made in my 1986 American Journal of Political Science article. Whereas I focused on how to avoid common mistakes in quantitative political sciences, Luskin clarifies ways to extract some useful information from usually problematic statistics: correlation coefficients, standardized coefficients, and especially R2. Since these three statistics are very closely related (and indeed deterministic functions of one another in some cases), I focus in this discussion primarily on R2, the most widely used and abused. Luskin also widens the discussion to various kinds of specification tests, a general issue I also address. In fact, as Beck (1991) reports, a large number of formal specification tests are just functions of R2, with differences among them primarily due to how much each statistic penalizes one for including extra parameters and fewer observations.

Quantitative political scientists often worry about model selection and specification, asking questions about parameter identification, autocorrelated or heteroscedastic disturbances, parameter constancy, variable choice, measurement error, endogeneity, functional forms, stochastic assumptions, and selection bias, among numerous others. These model specification questions are all important, but we may have forgotten why we pose them. Political scientists commonly give three reasons: (1) finding the "true" model, or the "full" explanation; (2) prediction; and (3) estimating specific causal effects. I argue here that (1) is used the most but useful the least; (2) is very useful but not usually in political science where forecasting is not often a central concern; and (3) correctly represents the goals of political scientists and should form the basis of most of our quantitative empirical work.},
	url = {http://gking.harvard.edu/files/abs/truth-abs.shtml},
	author = {Gary King}
}
@article {King91b,
	title = {Constituency Service and Incumbency Advantage},
	journal = {British Journal of Political Science},
	volume = {21},
	number = {1},
	year = {1991},
	month = {January},
	pages = {119{\textendash}128},
	abstract = {This Note addresses the long-standing discrepancy between scholarly support for the effect of constituency service on incumbency advantage and a large body of contradictory empirical evidence. I show first that many of the methodological problems noticed in past research reduce to a single methodological problem that is readily resolved. The core of this Note then provides among the first systematic empirical evidence for the constituency service hypothesis.

Specifically, an extra $10,000 added to the budget of the average state legislator gives this incumbent an additional 1.54 percentage points in the next election (with a 95\% confidence interval of 1.14 to 1.94 percentage points).},
	url = {http://gking.harvard.edu/files/abs/constit-abs.shtml},
	author = {Gary King}
}
@article {KinGel91,
	title = {Systemic Consequences of Incumbency Advantage in the U.S. House},
	journal = {American Journal of Political Science},
	volume = {35},
	number = {1},
	year = {1991},
	month = {February},
	pages = {110{\textendash}138},
	abstract = {The dramatic increase in the electoral advantage of incumbency has sparked widespread interest among congressional researchers over the last 15 years. Although many scholars have studied the advantages of incumbency for incumbents, few have analyzed its effects on the underlying electoral system. We examine the influence of the incumbency advantage on two features of the electoral system in the U.S. House elections: electoral responsiveness and partisan bias. Using a district-level seats-votes model of House elections, we are able to distinguish systematic changes from unique, election-specific variations. Our results confirm the significant drop in responsiveness, and even steeper decline outside the South, over the past 40 years. Contrary to expectations, we find that increased incumbency advantage explains less than a third of this trend, indicating that some other unknown factor is responsible. Moreover, our analysis also reveals another dramatic pattern, largely overlooked in the congressional literature: in the 1940{\textquoteright}s and 1950{\textquoteright}s the electoral system was severely biased in favor of the Republican party. The system shifted incrementally from this severe Republican bias over the next several decades to a moderate Democratic bias by the mid-1980{\textquoteright}s. Interestingly, changes in incumbency advantage explain virtually all of this trend in partisan bias since the 1940{\textquoteright}s. By removing incumbency advantage and the existing configuration of incumbents and challengers analytically, our analysis reveals an underlying electoral system that remains consistently biased in favor of the Republican party. Thus, our results indicate that incumbency advantage affects the underlying electoral system, but contrary to conventional wisdom, this changes the trend in partisan bias more than electoral responsiveness.},
	url = {http://gking.harvard.edu/files/abs/sysconseq-abs.shtml},
	author = {Gary King and Andrew Gelman}
}
@article {King91e,
	title = {Calculating Standard Errors of Predicted Values based on Nonlinear Functional Forms},
	journal = {The Political Methodologist},
	volume = {4},
	number = {2},
	year = {1991},
	month = {Fall},
	author = {Gary King}
}
@article {King91c,
	title = {On Political Methodology},
	journal = {Political Analysis},
	volume = {2},
	year = {1991},
	pages = {1{\textendash}30},
	abstract = {"Politimetrics" (Gurr 1972), "polimetrics" (Alker 1975), "politometrics" (Hilton 1976), "political arithmetic" (Petty [1672] 1971), "quantitative Political Science (QPS)," "governmetrics," "posopolitics" (Papayanopoulos 1973), "political science statistics (Rai and Blydenburgh 1973), "political statistics" (Rice 1926). These are some of the names that scholars have used to describe the field we now call "political methodology." The history of political methodology has been quite fragmented until recently, as reflected by this patchwork of names. The field has begun to coalesce during the past decade; we are developing persistent organizations, a growing body of scholarly literature, and an emerging consensus about important problems that need to be solved. I make one main point in this article: If political methodology is to play an important role in the future of political science, scholars will need to find ways of representing more interesting political contexts in quantitative analyses. This does not mean that scholars should just build more and more complicated statistical models. Instead, we need to represent more of the essence of political phenomena in our models. The advantage of formal and quantitative approaches is that they are abstract representations of the political world and are, thus, much clearer. We need methods that enable us to abstract the right parts of the phenomenon we are studying and exclude everything superfluous. Despite the fragmented history of quantitative political analysis, a version of this goal has been voiced frequently by both quantitative researchers and their critics (Sec. 2). However, while recognizing this shortcoming, earlier scholars were not in the position to rectify it, lacking the mathematical and statistical tools and, early on, the data. Since political methodologists have made great progress in these and other areas in recent years, I argue that we are now capable of realizing this goal. In section 3, I suggest specific approaches to this problem. Finally, in section 4, I provide two modern examples, ecological inference and models of spatial autocorrelation, to illustrate these points.},
	url = {http://gking.harvard.edu/files/abs/polmeth-abs.shtml},
	author = {Gary King}
}
@article {King91d,
	title = {Stochastic Variation: A Comment on Lewis-Beck and Skalaban{\textquoteright}s {\textquoteright}The R-Square{\textquoteright}},
	journal = {Political Analysis},
	volume = {2},
	year = {1991},
	pages = {185{\textendash}200},
	abstract = {In an interesting and provocative article, Michael Lewis-Beck and Andrew Skalaban make an important contribution by emphasizing several philosophical issues in political methodology that have received too little attention from methodologists and quantitative researchers. These issues involve the role of systematic, and especially stochastic, variation in statistical models. After briefly discussing a few points of disagreement, hoping to reduce them to points of clarification, I turn to the philosophical issues. Examples with real data follow.},
	url = {http://gking.harvard.edu/files/abs/stoch-abs.shtml},
	author = {Gary King}
}
@article {King91e,
	title = {Replication data for: Constituency Service and Incumbency Advantage},
	year = {1991},
	note = {{\underline{hdl:1902.1/JTMXGSZXIZ} UNF:3:IE4ZSAs8ZzUK+fRXNbVvGw== Murray Research Archive [Distributor]}},
	author = {Gary King}
}
@article {King91f,
	title = {Replication data for: On Political Methodology},
	year = {1991},
	note = {{\underline{hdl:1902.1/KHTLSQXAEJ} Murray Research Archive [Distributor]}},
	author = {Gary King}
}
@article {GelKin90b,
	title = {Estimating Incumbency Advantage Without Bias},
	journal = {American Journal of Political Science},
	volume = {34},
	number = {4},
	year = {1990},
	month = {November},
	pages = {1142{\textendash}1164},
	abstract = {In this paper we prove theoretically and demonstrate empirically that all existing measures of incumbency advantage in the congressional elections literature are biased or inconsistent. We then provide an unbiased estimator based on a very simple linear regression model. We apply this new method to congressional elections since 1900, providing the first evidence of a positive incumbency advantage in the first half of the century.},
	url = {http://gking.harvard.edu/files/abs/inc-abs.shtml},
	author = {Andrew Gelman and Gary King}
}
@article {King90,
	title = {Electoral Responsiveness and Partisan Bias in Multiparty Democracies},
	journal = {Legislative Studies Quarterly},
	volume = {XV},
	number = {2},
	year = {1990},
	month = {May},
	pages = {159{\textendash}181},
	abstract = {Because the goals of local and national representation are inherently incompatible, there is an uncertain relationship between aggregates of citizen votes and the national allocation of legislative seats in almost all democracies. In particular electoral systems, this uncertainty leads to diverse configurations of electoral responsiveness and partisian bias, two fundamental concepts in empirical democratic theory. This paper unifies virtually all existing multiyear seats-votes models as special cases of a new general model. It also permits the first formalization of, and reliable method for empirically estimating, electoral responsiveness and partisian bias in electoral systems with any number of political parties. I apply this model to data from nine democratic countries, revealing clear patterns in responsiveness and bias across different types of electoral rules.},
	url = {http://gking.harvard.edu/files/abs/electresp-abs.shtml},
	author = {Gary King}
}
@article {AnsKin90,
	title = {Measuring the Consequences of Delegate Selection Rules in Presidential Nominations},
	journal = {Journal of Politics},
	volume = {52},
	number = {2},
	year = {1990},
	month = {May},
	pages = {609{\textendash}621},
	abstract = {In this paper, we formalize existing normative criteria used to judge presidential selection contests by modeling the translation of citizen votes in primaries and caucuses into delegates to the national party conventions. We use a statistical model that enables us to separate the form of electoral responsiveness in presidential selection systems, as well as the degree of bias toward each of the candidates. We find that (1) the Republican nomination system is more responsive to changes in citizen votes than the Democratic system; (2) non-PR primaries are always more responsive than PR primaries; (3) surprisingly, caucuses are more proportional than even primaries held under PR rules; (4) significant bias in favor of a candidate was a good prediction of the winner of the nomination contest. We also (5) evaluate the claims of Ronald Reagan in 1976 and Jesse Jackson in 1988 that the selection systems were substantially biased against their candidates. We find no evidence to support Reagan{\textquoteright}s claim, but substantial evidence that Jackson was correct.},
	url = {http://gking.harvard.edu/files/abs/pri-abs.shtml},
	author = {Stephen Ansolabehere and Gary King}
}
@article {GelKin90,
	title = {Estimating the Electoral Consequences of Legislative Redistricting},
	journal = {Journal of the American Statistical Association},
	volume = {85},
	number = {410},
	year = {1990},
	month = {June},
	pages = {274{\textendash}282},
	abstract = {We analyze the effects of redistricting as revealed in the votes received by the Democratic and Republican candidates for state legislature. We develop measures of partisan bias and the responsiveness of the composition of the legislature to changes in statewide votes. Our statistical model incorporates a mixed hierarchical Bayesian and non-Bayesian estimation, requiring simulation along the lines of Tanner and Wong (1987). This model provides reliable estimates of partisan bias and responsiveness along with measures of their variabilities from only a single year of electoral data. This allows one to distinguish systematic changes in the underlying electoral system from typical election-to-election variability.},
	url = {http://gking.harvard.edu/files/abs/svstat-abs.shtml},
	author = {Andrew Gelman and Gary King}
}
@article {KinAltBur90,
	title = {A Unified Model of Cabinet Dissolution in Parliamentary Democracies},
	journal = {American Journal of Political Science},
	volume = {34},
	number = {3},
	year = {1990},
	month = {August},
	pages = {846{\textendash}871},
	abstract = {The literature on cabinet duration is split between two apparently irreconcilable positions. The attributes theorists seek to explain cabinet duration as a fixed function of measured explanatory variables, while the events process theorists model cabinet durations as a product of purely stochastic processes. In this paper we build a unified statistical model that combines the insights of these previously distinct approaches. We also generalize this unified model, and all previous models, by including (1) a stochastic component that takes into account the censoring that occurs as a result of governments lasting to the vicinity of the maximum constitutional interelection period, (2) a systematic component that precludes the possibility of negative duration predictions, and (3) a much more objective and parsimonious list of explanatory variables, the explanatory power of which would not be improved by including a list of indicator variables for individual countries.},
	url = {http://gking.harvard.edu/files/abs/coal-abs.shtml},
	author = {Gary King and James Alt and Nancy Burns and Michael Laver}
}
@article {AnsKin90b,
	title = {Replication data for: Measuring the Consequences of Delegate Selection Rules in Presidential Nominations},
	year = {1990},
	note = {{\underline{hdl:1902.1/BUJXCEPXQK} UNF:3:OdFPcQcvfO5hc3WJ5ty8vQ== Murray Research Archive [Distributor]}},
	author = {Stephen Ansolabehere and Gary King}
}
@article {King89b,
	title = {Representation Through Legislative Redistricting: A Stochastic Model},
	journal = {American Journal of Political Science},
	volume = {33},
	number = {4},
	year = {1989},
	month = {November},
	pages = {787{\textendash}824},
	abstract = {This paper builds a stochastic model of the processes that give rise to observed patterns of representation and bias in congressional and state legislative elections. The analysis demonstrates that partisan swing and incumbency voting, concepts from the congressional elections literature, have determinate effects on representation and bias, concepts from the redistricting literature. The model shows precisely how incumbency and increased variability of partisan swing reduce the responsiveness of the electoral system and how partisan swing affects whether the system is biased toward one party or the other. Incumbency, and other causes of unresponsive representation, also reduce the effect of partisan swing on current levels of partisan bias. By relaxing the restrictive portions of the widely applied "uniform partisan swing" assumption, the theoretical analysis leads directly to an empirical model enabling one more reliably to estimate responsiveness and bias from a single year of electoral data. Applying this to data from seven elections in each of six states, the paper demonstrates that redistricting has effects in predicted directions in the short run: partisan gerrymandering biases the system in favor of the party in control and, by freeing up seats held by opposition party incumbents, increases the system{\textquoteright}s responsiveness. Bipartisan-controlled redistricting appears to reduce bias somewhat and dramatically to reduce responsiveness. Nonpartisan redistricting processes substantially increase responsiveness but do not have as clear an effect on bias. However, after only two elections, prima facie evidence for redistricting effects evaporate in most states. Finally, across every state and type of redistricting process, responsiveness declined significantly over the course of the decade. This is clear evidence that the phenomenon of "vanishing marginals," recognized first in the U.S. Congress literature, also applies to these different types of state legislative assemblies. It also strongly suggests that redistricting could not account for this pattern.},
	url = {http://gking.harvard.edu/files/abs/repstoch-abs.shtml},
	author = {Gary King}
}
@article {King89c,
	title = {Event Count Models for International Relations: Generalizations and Applications},
	journal = {International Studies Quarterly},
	volume = {33},
	number = {2},
	year = {1989},
	month = {June},
	pages = {123{\textendash}147},
	abstract = {International relations theorists tend to think in terms of continuous processes. Yet we observe only discrete events, such as wars or alliances, and summarize them in terms of the frequency of occurrence. As such, most empirical analyses in international relations are based on event count variables. Unfortunately, analysts have generally relied on statistical techniques that were designed for continuous data. This mismatch between theory and method has caused bias, inefficiency, and numerous inconsistencies in both theoretical arguments and empirical findings throughout the literature. This article develops a much more powerful approach to modeling and statistical analysis based explicity on estimating continuous processes from observed event counts. To demonstrate this class of models, I present several new statistical techniques developed for and applied to different areas of international relations. These include the influence of international alliances on the outbreak of war, the contagious process of multilateral economic sanctions, and reciprocity in superpower conflict. I also show how one can extract considerably more information from existing data and relate substantive theory to empirical analyses more explicitly with this approach.},
	url = {http://gking.harvard.edu/files/abs/ISQ33-abs.shtml},
	author = {Gary King}
}
@article {King89e,
	title = {A Seemingly Unrelated Poisson Regression Model},
	journal = {Sociological Methods and Research},
	volume = {17},
	number = {3},
	year = {1989},
	month = {February},
	pages = {235{\textendash}255},
	abstract = {This article introduces a new estimator for the analysis of two contemporaneously correlated endogenous event count variables. This seemingly unrelated Poisson regression model (SUPREME) estimator combines the efficiencies created by single equation Poisson regression model estimators and insights from "seemingly unrelated" linear regression models.},
	url = {http://gking.harvard.edu/files/abs/SMR17-abs.shtml},
	author = {Gary King}
}
@article {King89d,
	title = {Variance Specification in Event Count Models: From Restrictive Assumptions to a Generalized Estimator},
	journal = {American Journal of Political Science},
	volume = {33},
	number = {3},
	year = {1989},
	month = {August},
	pages = {762{\textendash}784},
	abstract = {This paper discusses the problem of variance specification in models for event count data. Event counts are dependent variables that can take on only nonnegative integer values, such as the number of wars or coups d{\textquoteright}etat in a year. I discuss several generalizations of the Poisson regression model, presented in King (1988), to allow for substantively interesting stochastic processes that do not fit into the Poisson framework. Individual models that cope with, and help analyze, heterogeneity, contagion, and negative contagion are each shown to lead to specific statistical models for event count data. In addition, I derive a new generalized event count (GEC) model that enables researchers to extract significant amounts of new information from existing data by estimating features of these unobserved substantive processes. Applications of this model to congressional challenges of presidential vetoes and superpower conflict demonstrate the dramatic advantages of this approach.},
	url = {http://gking.harvard.edu/files/abs/varspecec-abs.shtml},
	author = {Gary King}
}
@article {GelKin89,
	title = {Electoral Responsiveness in U.S. Congressional Elections, 1946-1986},
	journal = {Proceedings of the Social Statistics Section, American Statistical Association},
	year = {1989},
	pages = {208},
	author = {Andrew Gelman and Gary King}
}
@book {BraHarKin89,
	title = {The Presidency in American Politics},
	year = {1989},
	publisher = {New York University Press},
	organization = {New York University Press},
	address = {New York and London},
	author = {Paul Brace and Christine Harrington and Gary King}
}
@book {King89,
	title = {Unifying Political Methodology: The Likelihood Theory of Statistical Inference},
	year = {1989},
	publisher = {Michigan University Press},
	organization = {Michigan University Press},
	address = {Ann Arbor},
	author = {Gary King}
}
@article {King88,
	title = {Statistical Models for Political Science Event Counts: Bias in Conventional Procedures and Evidence for The Exponential Poisson Regression Model},
	journal = {American Journal of Political Science},
	volume = {32},
	number = {3},
	year = {1988},
	month = {August},
	pages = {838-863},
	abstract = {This paper presents analytical, Monte Carlo, and empirical evidence on models for event count data. Event counts are dependent variables that measure the number of times some event occurs. Counts of international events are probably the most common, but numerous examples exist in every empirical field of the discipline. The results of the analysis below strongly suggest that the way event counts have been analyzed in hundreds of important political science studies have produced statistically and substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, and other problems result from the unknowing application of two common methods that are without theoretical justification or empirical unity in this type of data. I show that the exponential Poisson regression (EPR) model provides analytically, in large samples, and empirically, in small, finite samples, a far superior model and optimal estimator. I also demonstrate the advantage of this methodology in an application to nineteenth-century party switching in the U.S. Congress. Its use by political scientists is strongly encouraged.},
	url = {http://gking.harvard.edu/files/abs/epr-abs.shtml},
	author = {Gary King}
}
@book {KinRag88,
	title = {The Elusive Executive: Discovering Statistical Patterns in the Presidency},
	year = {1988},
	publisher = {Congressional Quarterly Press},
	organization = {Congressional Quarterly Press},
	address = {Washington, D.C},
	author = {Gary King and Lyn Ragsdale}
}
@article {King87,
	title = {Presidential Appointments to the Supreme Court: Adding Systematic Explanation to Probabilistic Description},
	journal = {American Politics Quarterly},
	volume = {15},
	number = {3},
	year = {1987},
	month = {July},
	pages = {373{\textendash}386},
	abstract = {Three articles, published in the leading journals of three disciplines over the last five decades, have each used the Poisson probability distribution to help describe the frequency with which presidents were able to appoint United States Supreme Court Justices. This work challenges these previous findings with a new model of Court appointments. The analysis demonstrates that the number of appointments a president can expect to make in a given year is a function of existing measurable variables.},
	url = {http://gking.harvard.edu/files/abs/sct-abs.shtml},
	author = {Gary King}
}
@article {BroKin87,
	title = {Seats, Votes, and Gerrymandering: Measuring Bias and Representation in Legislative Redistricting},
	journal = {Law and Policy},
	volume = {9},
	number = {3},
	year = {1987},
	month = {July},
	pages = {305{\textendash}322},
	abstract = {The Davis v. Bandemer case focused much attention on the problem of using statistical evidence to demonstrate the existence of political gerrymandering. In this paper, we evaluate the uses and limitations of measures of the seat-votes relationship in the Bandemer case. We outline a statistical method we have developed that can be used to estimate bias and the form of representation in legislative redistricting. We apply this method to Indiana State House and Senate elections for the period 1972 to 1984 and demonstrate a maximum bias 6.2\% toward the Republicans in the House and a 2.8\% bias in the Senate.},
	url = {http://gking.harvard.edu/files/abs/LP9-abs.shtml},
	author = {Robert X Browning and Gary King}
}
@article {KinBro87,
	title = {Democratic Representation and Partisan Bias in Congressional Elections},
	journal = {American Political Science Review},
	volume = {81},
	number = {4},
	year = {1987},
	month = {December},
	pages = {1252{\textendash}1273},
	abstract = {The translation of citizen votes into legislative seats is of central importance in democratic electoral systems. It has been a longstanding concern among scholars in political science and in numerous other disciplines. Through this literature, two fundamental tenets of democratic theory, partisan bias and democratic representation, have often been confused. We develop a general statistical model of the relationship between votes and seats and separate these two important concepts theoretically and empirically. In so doing, we also solve several methodological problems with the study of seats, votes and the cube law. An application to U.S. congressional districts provides estimates of bias and representation for each state and deomonstrates the model{\textquoteright}s utility. Results of this application show distinct types of representation coexisting in U.S. states. Although most states have small partisan biases, there are some with a substantial degree of bias.},
	url = {http://gking.harvard.edu/files/abs/sv-abs.shtml},
	author = {Gary King and Robert X Browning}
}
@article {KinMer86,
	title = {The Development of Political Activists: A Model of Early Learning},
	journal = {Social Science Quarterly},
	volume = {67},
	number = {3},
	year = {1986},
	month = {September},
	pages = {473{\textendash}490},
	abstract = {An analysis of panel data reveals the unique importance of early learning to the development of political activism among Americans. A combination of two learning models-- the frequently used crystallization model and the rarely analyzed sensitization model-- is advanced as most appropriate for understanding political socialization and the development of political activism. The findings contribute to research on elite behavior and on the process of political socialization.},
	url = {http://gking.harvard.edu/files/abs/poliactiv-abs.shtml},
	author = {Gary King and Richard Merelman}
}
@article {King86c,
	title = {Political Parties and Foreign Policy: A Structuralist Approach},
	journal = {Political Psychology},
	volume = {7},
	number = {1},
	year = {1986},
	month = {March},
	pages = {83{\textendash}101},
	abstract = {This article introduces the theory and approach of structural anthropology and applies it to a problem in American political science. Through this approach, the "bipartisan foreign policy hypothesis" and that "two presidencies hypothesis" are reformulated and reconsidered. Until now participants in the debate over each have only rarely built on, or even cited, the other{\textquoteright}s research. An additional problem is that the widespread conventional wisdom in support of the two hypotheses is inconsistent with systematic scholarly analyses. This paper demonstrates that the two hypotheses are drawn from the same underlying structure. Each hypothesis and the theoretical model it implies is conceptually and empirically extended to take into account the differences between congressional leaders and members. Then, historical examples and statistical analyses of House roll call data are used to demonstrate that the hypotheses, while sometimes supported for the congressional members, are far more applicable to leadership decision making. Conclusions suggest that conventional wisdom be revised to take these differences into account.},
	url = {http://gking.harvard.edu/files/abs/PP7-abs.shtml},
	author = {Gary King}
}
@article {King86b,
	title = {The Significance of Roll Calls in Voting Bodies: A Model and Statistical Estimation},
	journal = {Social Science Research},
	volume = {15},
	year = {1986},
	month = {June},
	pages = {135{\textendash}152},
	abstract = {In the long history of legislative roll call analyses, there continues to exist a particularly troubling problem: There is no satisfactory method for measuring the relative importance or significance of individual roll calls. A measure of roll call significance would be interesting in and of itself, but many have realized that it could also substantially improve empirical research. The consequence of this situation is that hundreds of researchers risk heteroskedastic disturbances (resulting in inefficient estimates and biased standard errors and test statistics), are unable to appropriately choose the roll calls most suited to their theory (resulting in analyses that may not correctly test their theory), and often use methods that create more problems than they solve (resulting in selection bias, unrealistic weighting schemes, or relatively subjective measures). This article introduces a new method designed to meet these problems. Based on an application of Box-Tiao intervention analysis, the method extracts from observed voting participation scores the "revealed preferences" of legislators as a measure of roll call significance. Applying this method to roll calls from the U.S. Senate demonstrates the success of the method and suggests its utility in applied research.},
	url = {http://gking.harvard.edu/files/abs/SSR15-abs.shtml},
	author = {Gary King}
}
@article {King86,
	title = {How Not to Lie With Statistics: Avoiding Common Mistakes in Quantitative Political Science},
	journal = {American Journal of Political Science},
	volume = {30},
	number = {3},
	year = {1986},
	month = {August},
	pages = {666{\textendash}687},
	abstract = {This article identifies a set of serious theoretical mistakes appearing with troublingly high frequency throughout the quantitative political science literature. These mistakes are all based on faulty statistical theory or on erroneous statistical analysis. Through algebraic and interpretive proofs, some of the most commonly made mistakes are explicated and illustrated. The theoretical problem underlying each is highlighted, and suggested solutions are provided throughout. It is argued that closer attention to these problems and solutions will result in more reliable quantitative analyses and more useful theoretical contributions.},
	url = {http://gking.harvard.edu/files/abs/mist-abs.shtml},
	author = {Gary King}
}
@article {KinBen86,
	title = {Replication data for: The Stability of Partisan Identification in the U.S. House of Representatives, 1789-1984},
	year = {1986},
	note = {{\underline{hdl:1902.1/HINHTJQYFO} Murray Research Archive [Distributor]}},
	author = {Gary King and Gerald Benjamin}
}
